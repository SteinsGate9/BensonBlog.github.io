<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Lobster Two:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.jpg?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.jpg?v=5.1.4">


  <link rel="mask-icon" href="/images/favicon.ico?v=5.1.4" color="#222">





  <meta name="keywords" content="numerical stats," />










<meta name="description" content="Stats Learning Theory Course Slides Notes">
<meta name="keywords" content="numerical stats">
<meta property="og:type" content="article">
<meta property="og:title" content="Statistical Learning Theory">
<meta property="og:url" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;index.html">
<meta property="og:site_name" content="BeNsoN">
<meta property="og:description" content="Stats Learning Theory Course Slides Notes">
<meta property="og:locale" content="en">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225001640061.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;VqdvoazPSMInTQ2.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;OyqKboHkSQNDn75.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;HMS9BAGx8XZPTyQ.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;WEKaf4uoeIAlkc7.png">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6gy1g95jeljh2lj31c808wjt3.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6gy1g95jecq5rdj31cs0iuwgd.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201213904724.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;x7kQeKi8ybTrHwd.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123171055730.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;QrMq8YUoezv5lfF.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;nkh94C5FBHaPKdV.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123172003515.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191031153115435.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123172250177.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123172116906.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123173747296.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123173815260.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123174007296.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123174100478.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123172039645.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123173728461.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191031153310730.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;qYFxsHPloQND5w7.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;29vwoHBuIPEygGZ.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;3hfPieqtNvQcYol.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123174533026.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;31ckzfgGZ5vUaQm.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;QPOR4bGDZgHcIwM.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;WQcTBePXsGjtoEV.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123175242488.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123175306310.png">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g94lqs6o72j30p602kwem.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g94lrsn8ebj315g03iwes.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g94o1xyvb9j30qq08g3zb.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g94nwmwcxkj30rg04m3z2.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g94o4i1hlkj30r001u0t2.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g94nxbw79oj30ri02et8v.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g94qbhcivdj31570u07bt.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g94qc9fbg5j315u046aaj.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g94qc3sof4j316q046mya.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g94qcjj6jij315006aaaq.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g94qmpvb92j311e0bkmyh.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g94qpmgeaij317a0aajsy.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g94qpy0kq6j315k07wabg.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g95ma9grqbj315c0bgwgb.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g95maf8kpaj319a0f0wh3.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123175608269.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201215157244.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123175647270.png">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g95oj97tgqj316008kq5h.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g95ojr0phij315a0kggnt.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g95ojwr4yzj315w04a0tq.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201214112146.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191023001520129.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191023001529332.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123181111001.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123181123718.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123181155391.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123181259373.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123181710201.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123231644429.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123231808572.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123231659159.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123231853530.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191031230308646.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191124000519523.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191124000830105.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191124000859776.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191124001145579.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191124001241566.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191124001251576.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191124002128531.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191124002405533.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191124002546243.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191124002702537.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191124002717995.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191124002833322.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191124002856143.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191124002908944.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191124003023315.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191124003119956.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191124003221119.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201184553325.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201184643223.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201184701339.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201184717035.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201184802125.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201184807841.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201182534398.png">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g95ww91lpyj318u0p4ni8.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g95wwhbs4yj30o104iglz.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g95wxp8pjbj30pe0340u7.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g95wxx4mb1j317o0bm49j.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g95wy58truj30zs0k816h.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g95wx02eexj30o5012t8l.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g95r5on66nj316k04gwf6.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g95r5wpu0mj317009275x.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201182516400.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201182534398.png">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g95x6ig9taj30nu03tq38.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g962lasmw3j319007y7bn.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g962licechj318e05ogsc.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201220640230.png">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g962lpaos0j31820c4jwn.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g962veanruj30oo02at91.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g962mdqdqvj318o07ygoz.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g962mom5nrj30nw0fgt9n.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g962nvb719j30oa01v3yj.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123132232152.png">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g962o65ukwj30ox098gm9.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g95pm0mzxxj31760iqaeu.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g95px5f6gjj317m0jgwip.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g96450i51uj30o305pab5.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123134507969.png">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g964l986unj30ot0dm75n.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g964li6gk5j30ny031aai.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g964nsneufj30og02lt8y.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g964pd0whnj30nv04ljrp.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g964vbn2zuj30o40210su.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g964vlz3hcj30og02gglt.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123231131104.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123232017857.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123232034601.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123232045980.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123232056701.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123232108760.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123232117823.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123232241905.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123232312620.png">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g964pko8wij30ll03bjrf.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g964r6bma5j30mv08w74u.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123141515437.png">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g964rjujfgj30o703a74c.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123232332935.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123232347065.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123232359432.png">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g964sdwcymj30o3060dg7.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g964ys8qbkj30od02zgm9.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g964whyhyoj30n801j747.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123152755956.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123152820198.png">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g964yzm3znj30og02fdg3.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123151349836.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123151358537.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123151407404.png">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g964z6ewrbj30nh018aa2.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g964zbzzkoj30oe047aaz.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g964ziid51j30nx05daal.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191023091054642.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191031155626864.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191015161350400.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191023091839892.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191023091951058.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123121333880.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123121420040.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123121743529.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123121757398.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123122147883.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191204000442489.png">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g95o9tuhy7j315m07awg7.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g95o9zv0drj315y0560tu.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g95oaciywjj316y072wfk.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g95nkp8b07j31660ekwhc.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g95nlmbv8fj316k092jsx.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g95o8uz3txj315u06qgna.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g95olftvxlj316s0dqacl.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191123181529769.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222195231293.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222195242137.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222195345836.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222195405055.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222204848422.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222204914733.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222204900679.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222204925098.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222195546912.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222195613974.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222200726895.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222204458611.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222204505015.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222204714362.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222204737130.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222201320978.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222201332835.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200221225011848.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222205007587.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200221225028800.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222201636430.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200221225411456.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200221225427236.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200221225457278.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200221225508366.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200221225530194.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200221225609417.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200221230028280.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200221230123177.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200221230130238.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200221230327053.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200221230335139.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222011534422.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222011616320.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222011657225.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222012240963.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222012247357.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222012255444.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222012318683.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222220748859.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222221743143.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;Users&#x2F;huangbenson&#x2F;SteinsGate9.github.io&#x2F;source&#x2F;_posts&#x2F;stats_theory&#x2F;2020-02-25-20-07-18-image.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222221912680.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222222036084.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222224220219.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222224319598.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222224413100.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222224501055.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222230839198.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222225618355.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222225627831.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;eJ6NBMAcbIR9mFO.png">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g94uosvr98j30us09gt9p.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g94uoljm4lj30v00emta4.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;tgpQOvDowBsfxHN.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;q68lQbYzNG9xjSs.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222205804020.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222210035729.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222213437099.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;Qpn3gXHzTPF2aZY.png">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g94uo4jzd8j30uk030aak.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;NcJWjmECne4zbg1.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;5goRmWdDwpyv8hK.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;MtqP29VGAsvCU6B.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;VqbT2vfYJE9oSH3.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;zdSptJbWMUu9DQm.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;OzXTrZA4jmY9hVx.png">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g94un7bhjsj30wc0byta5.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;HR7Fxa9ZECuMVeS.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;h8gfUSourM4qYQw.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200224175649655.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200224175924749.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200224175943803.png">
<meta property="og:image" content="https:&#x2F;&#x2F;s2.ax1x.com&#x2F;2019&#x2F;11&#x2F;20&#x2F;MWp8JJ.png">
<meta property="og:image" content="https:&#x2F;&#x2F;s2.ax1x.com&#x2F;2019&#x2F;11&#x2F;20&#x2F;MWpAiQ.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200224185638711.png">
<meta property="og:image" content="https:&#x2F;&#x2F;s2.ax1x.com&#x2F;2019&#x2F;11&#x2F;20&#x2F;MWpfw8.png">
<meta property="og:image" content="https:&#x2F;&#x2F;s2.ax1x.com&#x2F;2019&#x2F;11&#x2F;20&#x2F;MWpOmV.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200224180800454.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200224182038092.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200224182053693.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200224182127376.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200224182207475.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;SwgJqitobc2Xvku.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;IrRWNKDMTS9mn6l.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200224181234861.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200224181252809.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;Jg5KwYC4clsp1LO.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200224222158590.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200224222211246.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200224225958578.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200224222037543.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;H13oW6M2znwQlKY.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200224233536888.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200224231301936.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200224231520882.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200224232457378.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200224232511561.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200224235941414.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;ksrwUoluGiF7jMI.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;eJiRtwo5M9AEkDN.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;oYLf9tT3VOjEwUW.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;IyrRtMfzo4mJelU.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;Wbrua1oidJILVPp.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;8CZfKqxytmOoYih.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.loli.net&#x2F;2019&#x2F;11&#x2F;20&#x2F;WUvX9rTzie2xRn4.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200224184216241.png">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g94kd7xe38j30sc024jrf.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g94kdj455cj30ok02q3ym.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g94kdp69btj30rs046aac.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200224184318763.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200224184511108.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200224184604328.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200224184619430.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200224184547938.png">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g94l6l7zfcj316e0380tf.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200224184845366.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200224184916504.png">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g94llq6xidj315w03ct92.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g94lf89bzoj314a05gwey.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g94lfwvlcxj315607y0tg.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g94lgbmkh1j30wi03oglu.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g94lgoido3j30y208oq3v.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g94lhpmmmmj316804wt9z.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g94ljbz53fj316o02gt99.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g94ljika20j317a0lon06.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6gy1g95jkwn25kj31d60hajuf.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200224180955389.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222013732650.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222013803270.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222013819229.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201002514743.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201002702582.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201002926764.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201003036044.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201003041939.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201003332732.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201003459205.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201003651075.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201003734364.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201003748527.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201003923496.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201003837128.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201003912925.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201003941978.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201003947318.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201004019825.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201004151416.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201004524088.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201004602624.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201004933939.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201005049589.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201005108232.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201005102049.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222233335780.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222233348331.png">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6gy1g95jk8garjj31ca0ma79u.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;Users&#x2F;huangbenson&#x2F;SteinsGate9.github.io&#x2F;source&#x2F;_posts&#x2F;stats_theory&#x2F;2020-02-25-19-57-45-image.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223132344665.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223005339040.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223135926230.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223140003765.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223140009293.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223143507596.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223143520591.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223175311247.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223175326033.png">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g95mj6tnn0j318o0640w1.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200221222911920.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200221223315528.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223180704186.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223190404515.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223190413981.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223190508260.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223190854106.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223191622978.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223200711546.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223200749655.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223200839235.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223201104710.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223201114398.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223201208033.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223201724406.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223201738461.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223205257326.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223210113859.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223213440569.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223213602551.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200224172850709.png">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6gy1g95jox8hk9j311i0u042p.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222142426917.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222142444145.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225003311056.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225003414890.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225003422240.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225003719069.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225003724934.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225011659794.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225011705184.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225011723744.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225112656786.png">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6gy1g95jsuxibxj316205wgml.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g95lgbda3bj319u0h6dj1.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g95ldb2stkj31ac05g40g.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g95m3zu4mkj316c054mxx.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g95l4x2rifj316a05ugml.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g95l513c30j315i0f0q57.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225003955539.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225004947206.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225004925903.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225005003342.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225005020613.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225005111311.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225005121597.png">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6gy1g95jtgz619j31880kmad7.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g95luzezzcj31b00d0dml.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6gy1g95jtonm5mj316u0by0vb.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225013323947.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225013259752.png">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g95mln63k6j316407o3zu.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g95n9q6myzj30oc0dy0tu.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;large&#x2F;006y8mN6ly1g95nakqg24j316i09cdi1.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222122909121.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222122917107.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222123146373.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222123310677.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222125318556.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222130719665.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223164351810.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222130730162.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222130739720.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222130754156.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222133419622.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222130914042.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223152634740.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223152655567.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223152805731.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223152923846.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223153049995.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223153218136.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223155718361.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223164440671.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223155841019.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223160203805.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223160309988.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223160953025.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223161022010.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223161028965.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223161239385.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223161228973.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223174438226.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223174429314.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223174450572.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223161312117.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223162719177.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223162812376.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200223162823508.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222141214352.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222141647500.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222141702131.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200222141724070.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201122002694.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201122031211.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201122107491.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201122121943.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201122137270.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201122348043.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201122412699.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201122650989.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201122704886.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201122713787.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201122721151.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201122735232.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201122757600.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201122802902.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201122545882.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201122551154.png">
<meta property="og:image" content="https:&#x2F;&#x2F;i.imgur.com&#x2F;cFQb1Iu.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201123134289.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201123429030.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201123437316.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201123540209.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201123550908.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201123710276.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201123719008.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201123912143.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201123922167.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201123926995.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201124039466.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201124057423.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201124129466.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201124244543.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201124220732.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201124322392.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201124336415.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201124425578.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201124518711.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201124541487.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201124550283.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201124651096.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201124658605.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201124739337.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201124754049.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20191201124759841.png">
<meta property="og:image" content="file:&#x2F;&#x2F;&#x2F;Users&#x2F;huangbenson&#x2F;SteinsGate9.github.io&#x2F;source&#x2F;_posts&#x2F;stats_theory&#x2F;2020-02-25-15-11-35-image.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225153540498.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225154411788.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225153134179.png">
<meta property="og:image" content="file:&#x2F;&#x2F;&#x2F;Users&#x2F;huangbenson&#x2F;SteinsGate9.github.io&#x2F;source&#x2F;_posts&#x2F;stats_theory&#x2F;2020-02-25-15-29-41-image.png">
<meta property="og:image" content="file:&#x2F;&#x2F;&#x2F;Users&#x2F;huangbenson&#x2F;SteinsGate9.github.io&#x2F;source&#x2F;_posts&#x2F;stats_theory&#x2F;2020-02-25-15-30-09-image.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225153219463.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225154003978.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225162329933.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225154028331.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225154108953.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225154300172.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225154310804.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225154437415.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225154449645.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225154515227.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225154544196.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225181941793.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225183525606.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225192715651.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225192738095.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225192745259.png">
<meta property="og:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225192730646.png">
<meta property="og:updated_time" content="2020-02-28T15:26:18.810Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;steinsgate9.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;stats_theory&#x2F;image-20200225001640061.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: 'FV65T83Q2V',
      apiKey: '5008cf0478de4ac53102baceee722a4d',
      indexName: 'steinsgate9',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://SteinsGate9.github.io/2019/11/19/stats_theory/"/>





  <title>Statistical Learning Theory | BeNsoN</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
<!--     <a href="https://github.com/SteinsGate9" target="_blank" rel="noopener" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style> -->

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta custom-logo">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">BeNsoN</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Live Long, Play Hard.</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            About
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" target="_blank" rel="noopener" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  
  <div class="algolia-popup popup search-popup">
    <div class="algolia-search">
      <div class="algolia-search-input-icon">
        <i class="fa fa-search"></i>
      </div>
      <div class="algolia-search-input" id="algolia-search-input"></div>
    </div>

    <div class="algolia-results">
      <div id="algolia-stats"></div>
      <div id="algolia-hits"></div>
      <div id="algolia-pagination" class="algolia-pagination"></div>
    </div>

    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>




    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://SteinsGate9.github.io/2019/11/19/stats_theory/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Benson">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://tva1.sinaimg.cn/large/006y8mN6gy1g96p07mbexj30uf0u0npd.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BeNsoN">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Statistical Learning Theory</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-11-19T12:50:43+08:00">
                2019-11-19
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/mathematics/" itemprop="url" rel="index">
                    <span itemprop="name">mathematics</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv">total read
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>times
            </span>
          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  6.4k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  38
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>Stats Learning Theory Course Slides Notes</p>
<p><img src="/2019/11/19/stats_theory/image-20200225001640061.png"></p>
<a id="more"></a>
<h1 id="risk-theory">1. risk theory</h1>
<h2 id="learning-algorithm-loss-functions-1-1-3-1">1.1. learning algorithm &amp; loss functions (1-1, 3-1)</h2>
<p>f -&gt; h, L -&gt; R</p>
<ul>
<li><p><strong>Def</strong>: learning algorithm Ln (3-1)</p>
<p><strong>Usage</strong>: from data to algorithms space</p>
<p><img src="https://i.loli.net/2019/11/20/VqdvoazPSMInTQ2.png"></p>
<ul>
<li><p><strong>Def</strong>: weakly consistent / strongly consistent （3-1)</p>
<p><img src="https://i.loli.net/2019/11/20/OyqKboHkSQNDn75.png"></p>
<p><strong>Note</strong>: <span class="math inline">\(\hat{h_n}\)</span> is random variable since it completely depends on iid data when <span class="math inline">\(L_n\)</span> is fixed. (3-1)</p>
<p><img src="https://i.loli.net/2019/11/20/HMS9BAGx8XZPTyQ.png"></p></li>
</ul></li>
<li><p><strong>Def</strong>: loss (loss function) (1-1)</p>
<p><img src="https://i.loli.net/2019/11/20/WEKaf4uoeIAlkc7.png"></p>
<p><strong>Example</strong>: the relation of max likelihood &amp; loss function (1-10)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g95jeljh2lj31c808wjt3.jpg"></p>
<p><strong>Example</strong>: (1-10)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g95jecq5rdj31cs0iuwgd.jpg"></p></li>
</ul>
<h2 id="loss-functions">1.2. loss functions</h2>
<p><img src="/2019/11/19/stats_theory/image-20191201213904724.png"></p>
<h3 id="e-f-loss-1-1-3-1">1.2.1. E-f-loss (1-1, 3-1)</h3>
<p><strong>Goal</strong>: we analyze what decide a good f function and what are the bounds between f and f star.</p>
<ul>
<li><p><strong>Def</strong>: E-f-loss (generalized expected risk) (1-1)</p>
<p><img src="https://i.loli.net/2019/11/20/x7kQeKi8ybTrHwd.png"></p>
<p>(3-1)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123171055730.png"></p></li>
<li><p><strong>Def</strong>: min-E-f-loss (minimal generalized expected risk) (1-1)</p>
<p><img src="https://i.loli.net/2019/11/20/QrMq8YUoezv5lfF.png"></p>
<ul>
<li><p><strong>Theorm</strong>: min-E-f-loss for binary classification is bayes classification</p>
<p>(1-1)</p>
<p><img src="https://i.loli.net/2019/11/20/nkh94C5FBHaPKdV.png"></p>
<p>(3-2)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123172003515.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20191031153115435.png"></p>
<p><strong>Proof</strong>: (3-2)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123172250177.png"></p></li>
</ul></li>
<li><p><strong>Theorem</strong>: min-E-f-loss for binary classification(3-2)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123172116906.png"></p>
<p><strong>Proof</strong>: (3-2)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123173747296.png"></p>
<p><strong>Note</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20191123173815260.png"></p></li>
<li><p><strong>Def</strong>: other form of bayes (3-2)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123174007296.png"></p>
<p><strong>Note</strong>: figure of bayes risk (from 3-2)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123174100478.png"></p></li>
<li><p><strong>Theorm</strong>: E-f-loss - min-E-f-loss (1-1, 3-1)</p>
<p><strong>Usage</strong>: find f to estimate f*</p>
<p>(3-2)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123172039645.png"></p>
<p><strong>Proof</strong>:(3-2)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123173728461.png"></p>
<p><strong>Note</strong>:(3-2)</p>
<p><img src="/2019/11/19/stats_theory/image-20191031153310730.png"></p>
<p>(1-1)</p>
<p><img src="https://i.loli.net/2019/11/20/qYFxsHPloQND5w7.png"></p>
<p><strong>Proof</strong>: (1-1)</p>
<p><img src="https://i.loli.net/2019/11/20/29vwoHBuIPEygGZ.png"></p></li>
</ul>
<h3 id="e-f_--loss-plug-in-classifiers-1-1-3-2">1.2.2. E-$f_ $-loss (plug-in classifiers) (1-1, 3-2)</h3>
<p><strong>Goal</strong>: we analyze the usage of plug-in classifiers.</p>
<ul>
<li><p><strong>Def</strong>: Plug-in classifier: swe can see that <span class="math inline">\(\eta\)</span> is what we what to estimate, the goal turns to find a better <span class="math inline">\(\eta\)</span>. (1-1)</p>
<p><img src="https://i.loli.net/2019/11/20/3hfPieqtNvQcYol.png"></p>
<p>(3-2)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123174533026.png"></p></li>
<li><p><strong>Theorem</strong>: E-<span class="math inline">\(f_\hat{\eta}\)</span>-loss - min-E-f-loss (1-1)</p>
<p><strong>Usage</strong>: when using <span class="math inline">\(\hat{\eta}\)</span> to find best f, instead of directly finding f, what's the bound?</p>
<p>(1-1)</p>
<p><img src="https://i.loli.net/2019/11/20/31ckzfgGZ5vUaQm.png"></p>
<p><strong>Proof</strong>: (1-1)</p>
<p><img src="https://i.loli.net/2019/11/20/QPOR4bGDZgHcIwM.png"></p>
<p><strong>Note</strong>: figure / pros &amp; cons to use <span class="math inline">\(\hat{\eta}\)</span> to minimize E loss (1-1)</p>
<p><img src="https://i.loli.net/2019/11/20/WQcTBePXsGjtoEV.png"></p>
<p>(3-2)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123175242488.png"></p>
<p><strong>Note</strong>: (3-2)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123175306310.png"></p></li>
</ul>
<h3 id="e-f-newloss-1-6-1-7">1.2.3. E-f-newloss (1-6, 1-7)</h3>
<p><strong>Goal</strong>: we analyze usage of different loss functions.</p>
<ul>
<li><p><strong>Def</strong>: E-f-newloss</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g94lqs6o72j30p602kwem.jpg"></p>
<p><strong>Note</strong>: SVM</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g94lrsn8ebj315g03iwes.jpg"></p></li>
<li><p><strong>Qua</strong>: =&gt; elaboration</p>
<p><strong>Usage</strong>: we can minimize E-f-newloss by minimizing the follows at every x/</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g94o1xyvb9j30qq08g3zb.jpg"></p></li>
<li><p><strong>Def</strong>: new elements to compute min-E-f-newloss, <span class="math inline">\(H(\eta)\)</span> means the lowest point of all f, all x.</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g94nwmwcxkj30rg04m3z2.jpg"></p>
<p><strong>Note</strong>: SVM</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g94o4i1hlkj30r001u0t2.jpg"></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g94nxbw79oj30ri02et8v.jpg"></p></li>
<li><p><strong>Qua</strong>: when <span class="math inline">\(\phi\)</span> is convex =&gt; <span class="math inline">\(H ^{-}(\eta) =\phi(0)\)</span></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g94qbhcivdj31570u07bt.jpg"></p></li>
<li><p><strong>Def</strong>: classification - calibrated</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g94qc9fbg5j315u046aaj.jpg"></p>
<p><strong>Example</strong>:</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g94qc3sof4j316q046mya.jpg"></p>
<ul>
<li><p><strong>Qua</strong>: necc &amp; suff<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g94qcjj6jij315006aaaq.jpg"></p>
<p><strong>Proof</strong>: too long</p></li>
<li><p><strong>Qua</strong>: real gap is bounded by E-f-newcalibratedloss</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g94qmpvb92j311e0bkmyh.jpg"></p>
<p><strong>Proof</strong>: too long</p>
<p><strong>Example</strong>: in the case of SVM, <span class="math inline">\(\phi\)</span> is a given convex function, therefore we can conpute <span class="math inline">\(H(\phi)\)</span> and yield a upper bound for the real gap. which means when training performs good, the result will most likely be the same.</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g94qpmgeaij317a0aajsy.jpg"></p>
<p><strong>Example</strong>: adaboost is the same, <span class="math inline">\(\phi(a)\)</span> is convex and we can again use theorem 2.2</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g94qpy0kq6j315k07wabg.jpg"></p>
<p><strong>Example</strong>: adaboost(1-13), a more detailed explaination, <span class="math inline">\(\phi\)</span> is class-calibrated, so we can use theorem 2.2.</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g95ma9grqbj315c0bgwgb.jpg"></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g95maf8kpaj319a0f0wh3.jpg"></p></li>
</ul></li>
</ul>
<h3 id="em-f-loss-1-213-3-345">1.2.4. Em-f-loss (1-2,13, 3-3,4,5)</h3>
<ul>
<li><p><strong>Def</strong>: Em-f-loss</p>
<p>(3-3)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123175608269.png"></p>
<p>(3-4) the best Em-f-loss h</p>
<p><img src="/2019/11/19/stats_theory/image-20191201215157244.png"></p>
<p><strong>Note</strong>: (3-3)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123175647270.png"></p></li>
<li><p><strong>Qua</strong>: Em-f-loss converge to E-f-loss (1-13)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g95oj97tgqj316008kq5h.jpg"></p>
<p><strong>Proof</strong>: why E(empirical risk) = generalized : E(h) is actually P(y|x), so $E((h))=_n E(1(...))/n=_n p/n = p $</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g95ojr0phij315a0kggnt.jpg"></p>
<p><strong>Note</strong>: (1-13)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g95ojwr4yzj315w04a0tq.jpg"></p></li>
</ul>
<h4 id="empirical-bounds-3-345">1.2.4.1. empirical bounds (3-3,4,5)</h4>
<p><strong>Goal</strong>: In this chapter we'll discover the bounds for empirical loss, or in other words, how well can we train the algorithm if we use empirical algorithms?</p>
<p>This is the whole picture: that we use em-loss to estimate the loss function.During this section, we'll use different tools to bound between <span class="math inline">\(\hat{E}\)</span> and E, such as VC &amp; hoeffding bound, etc. (1-18)<img src="/2019/11/19/stats_theory/image-20191201214112146.png"></p>
<h5 id="uniform-deviation-bound">1.2.4.1.1. uniform deviation bound</h5>
<p><strong>Goal</strong>: if we have bound generated by EPT, we have a goal proved.</p>
<ul>
<li><p><strong>Theorm</strong>:: =&gt; Em-f-loss - E-f-loss, uniform deviation bound (bound the performance of em)(not know h)(3-4)</p>
<p><img src="/2019/11/19/stats_theory/image-20191023001520129.png"></p>
<p><strong>Proof</strong>: (3-4)</p>
<p><img src="/2019/11/19/stats_theory/image-20191023001529332.png"></p>
<p><strong>note</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20191123181111001.png"></p></li>
<li><p><strong>Theorm</strong>: =&gt; E-fem-loss bound case 2 unifrom devia bound (3-4)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123181123718.png"></p>
<p><strong>Note</strong>:(3-4)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123181155391.png"></p></li>
<li><p><strong>Theorm</strong>: =&gt; E-fem-loss probability bound2 (3-4)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123181259373.png"></p></li>
<li><p><strong>Theorem</strong> =&gt; Em-f-loss bound 3, if zero error case (3-4)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123181710201.png"></p></li>
</ul>
<h5 id="vc-bound">1.2.4.1.2. vc bound</h5>
<p>the bound of EPT generated by vc</p>
<ul>
<li><p><strong>Theorm</strong>:: =&gt; Em-f-loss vc bound 4 (3-5)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123231644429.png"></p>
<p><strong>Note</strong>: that the right side can be bounded</p>
<p><img src="/2019/11/19/stats_theory/image-20191123231808572.png"></p>
<ul>
<li><p><strong>Corollary</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20191123231659159.png"></p>
<p><strong>Proof</strong>: follows from theorem using an argument like that when <span class="math inline">\(|H|&lt; \infty\)</span></p></li>
<li><p><strong>Corollary</strong>: this is obvious from theorem.</p>
<p><img src="/2019/11/19/stats_theory/image-20191123231853530.png"></p></li>
</ul></li>
<li><p><strong>Theorm</strong>: =&gt; learnable (3-5)</p>
<p><img src="/2019/11/19/stats_theory/image-20191031230308646.png"></p></li>
</ul>
<h5 id="bound-improvement-by-approximation-error">1.2.4.1.3. bound improvement by approximation error</h5>
<h6 id="uap-approximate-error">1.2.4.1.3.1. UAP &amp; approximate error</h6>
<ul>
<li><p><strong>Intro</strong>: decomposition to est ( discussed in above, basically random) + app ( not discussed before, decide by algorithm pool H). In this lecture we'll talk about app error (3-6)</p>
<p><img src="/2019/11/19/stats_theory/image-20191124000519523.png"></p>
<p><strong>Def</strong>: universal approximation property (3-6)</p>
<p><strong>Usage</strong>: UAP is when H pool grows, the approximation error converge to 0.</p>
<p><img src="/2019/11/19/stats_theory/image-20191124000830105.png"></p>
<p><strong>Note</strong>:(3-6)</p>
<p><img src="/2019/11/19/stats_theory/image-20191124000859776.png"></p></li>
<li><p><strong>Theorem</strong>: special Hk consist of piecewise constant classifiers =&gt; (3-6)</p>
<p><img src="/2019/11/19/stats_theory/image-20191124001145579.png"></p>
<p><strong>Example</strong>:(3-6)</p>
<p><img src="/2019/11/19/stats_theory/image-20191124001241566.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20191124001251576.png"></p></li>
</ul>
<h6 id="sieve-estimators">1.2.4.1.3.2. sieve estimators</h6>
<ul>
<li><p><strong>Def</strong>: best estimate classifier from H (3-6)</p>
<p><img src="/2019/11/19/stats_theory/image-20191124002128531.png"></p></li>
<li><p><strong>Intro &amp; Def</strong>:Strong Universally Consistent, why and what is a sieve estimator, basically to let the whole thing go to 0 simutaniously. (3-6)</p>
<p><img src="/2019/11/19/stats_theory/image-20191124002405533.png"></p></li>
<li><p><strong>Theorem</strong>: UAP &amp; k(n) sequence =&gt; (3-6)<img src="/2019/11/19/stats_theory/image-20191124002546243.png"></p>
<p><strong>Proof</strong>: (3-6)</p>
<p><img src="/2019/11/19/stats_theory/image-20191124002702537.png"></p>
<ul>
<li><p><strong>Corollary</strong>: similar to theorem =&gt; (3-6)</p>
<p><img src="/2019/11/19/stats_theory/image-20191124002717995.png"></p>
<p><strong>Note</strong>: (3-6)</p>
<p><img src="/2019/11/19/stats_theory/image-20191124002833322.png"></p></li>
</ul></li>
<li><p><strong>Theorem</strong>: UAP + VC bound =&gt; (3-6)</p>
<p><img src="/2019/11/19/stats_theory/image-20191124002856143.png"></p>
<p><strong>Proof</strong>:(3-6)</p>
<p><img src="/2019/11/19/stats_theory/image-20191124002908944.png"></p></li>
</ul>
<h6 id="rate-of-convergence">1.2.4.1.3.3. rate of convergence</h6>
<ul>
<li><p><strong>Def</strong>: rate of convergence(3-6)<img src="/2019/11/19/stats_theory/image-20191124003023315.png"></p></li>
<li><p><strong>Theorem</strong>:(3-6)<img src="/2019/11/19/stats_theory/image-20191124003119956.png"></p>
<p><strong>Note</strong>:(3-6)</p>
<p><img src="/2019/11/19/stats_theory/image-20191124003221119.png"></p>
<p><strong>Example</strong>: box-counting class (3-6)</p></li>
</ul>
<h5 id="rademacher-averages-bound-1-18">1.2.4.1.4. Rademacher averages bound (1-18)</h5>
<ul>
<li><p><strong>Def</strong>: RA</p>
<p><strong>Usage</strong>: how well function class F align to random directions, as well as to provide a upper bound</p>
<p><img src="/2019/11/19/stats_theory/image-20191201184553325.png"></p>
<ul>
<li><p><strong>Qua</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20191201184643223.png"></p></li>
<li><p><strong>Qua</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20191201184701339.png"></p>
<p><strong>Note</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20191201184717035.png"></p></li>
<li><p><strong>Note</strong>: concern with loss function</p>
<p><img src="/2019/11/19/stats_theory/image-20191201184802125.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20191201184807841.png"></p></li>
</ul></li>
</ul>
<h4 id="empirical-processing-theory">1.2.4.2. empirical processing theory</h4>
<p><strong>Goal</strong>: to make Em &amp; E close enough, Empirical Process theory allows us to prove uniform convergence laws of various kinds. One of the ways to start Empirical Process theory is from the Glivenko-Cantelli theorem. This leads to the question of whether the same idea can be generalized to other function classes.</p>
<h5 id="glivenko-cantelli">1.2.4.2.1. Glivenko-Cantelli</h5>
<p>This is where we'll start, by proving GC and go further, GC means that for certain class of mapping G, we have the following: and in the chapter after we'll generlize G .</p>
<p><img src="/2019/11/19/stats_theory/image-20191201182534398.png"></p>
<h6 id="empirical-distribution-function-book-2-2">1.2.4.2.1.1. empirical distribution function (book, 2-2)</h6>
<ul>
<li><p><strong>Def</strong>: empirical distribution function</p>
<p><strong>Usage</strong>: is a statistic</p>
<p>(book)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g95ww91lpyj318u0p4ni8.jpg"></p>
<p>(2-2)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g95wwhbs4yj30o104iglz.jpg"></p></li>
<li><p><strong>Qua</strong>: =&gt; range (book)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g95wxp8pjbj30pe0340u7.jpg"></p></li>
<li><p><strong>Qua</strong>: =&gt; other interpretation(book)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g95wxx4mb1j317o0bm49j.jpg"></p></li>
<li><p><strong>Qua</strong>: =&gt; limits (book)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g95wy58truj30zs0k816h.jpg"></p>
<p>(2-2)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g95wx02eexj30o5012t8l.jpg"></p></li>
</ul>
<h6 id="gc-class-theory-1-16-2-2">1.2.4.2.1.2. GC class &amp; theory (1-16, 2-2)</h6>
<ul>
<li><p><strong>Def</strong>: GC class (1-16)</p>
<p><strong>Usage</strong>: difference between P-GC is the supPn.</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g95r5on66nj316k04gwf6.jpg"></p></li>
<li><p><strong>Theorem</strong>: GC theory</p>
<p>(1-16)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g95r5wpu0mj317009275x.jpg"></p>
<p>(1-17)</p>
<p><img src="/2019/11/19/stats_theory/image-20191201182516400.png"></p>
<p><strong>Note</strong>: this is equivalent(1-17)</p>
<p><img src="/2019/11/19/stats_theory/image-20191201182534398.png"></p>
<p>(2-2)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g95x6ig9taj30nu03tq38.jpg"></p>
<p>(book)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g962lasmw3j319007y7bn.jpg"></p>
<p><strong>Note</strong>: Fn and F is very close when n is large</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g962licechj318e05ogsc.jpg"></p></li>
</ul>
<h5 id="generalized-glivenko-cantelli-1-1314-2-6">1.2.4.2.2. generalized Glivenko-Cantelli (1-13,14, 2-6)</h5>
<h6 id="symmetrization-lemma-2-5">1.2.4.2.2.1. symmetrization lemma (2-5)</h6>
<p>Lemma in this section is important to generalize GC.</p>
<p><img src="/2019/11/19/stats_theory/image-20191201220640230.png"></p>
<ul>
<li><p><strong>Def</strong> : recall distribution function (2-5)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g962lpaos0j31820c4jwn.jpg"></p></li>
<li><p><strong>Def</strong>: Rademacher variables (2-5)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g962veanruj30oo02at91.jpg"></p></li>
<li><p><strong>Lemma</strong>: =&gt; first symmtry bound the Fn-F(t) (2-5)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g962mdqdqvj318o07ygoz.jpg"></p>
<p><strong>Usage</strong>: using two indenpendent stochastic process to set upper bound for one .</p>
<p><strong>Example</strong>: (2-5)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g962mom5nrj30nw0fgt9n.jpg"></p></li>
<li><p><strong>Lemma</strong>: second symmtry =&gt; aallowsustoreplacethedifferenceFn−Fn′ withasingleempiricalquantity consisting of n observations. We can further bound the latter so that the bound is independent of the data ξ. (2-5)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g962nvb719j30oa01v3yj.jpg"></p>
<p><strong>Usage</strong>：using independent variables to set upper bound for GC (2-5)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123132232152.png"></p>
<p><strong>Proof</strong>: (2-5)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g962o65ukwj30ox098gm9.jpg"></p></li>
</ul>
<h6 id="generalized-gc">1.2.4.2.2.2. generalized GC</h6>
<p>Using Lemma above to prove GC</p>
<p><strong>Goal</strong>: for every F, make Em &amp; E close enough. we can first generalize the Glivenko-Cantelli theorem (a special F space).</p>
<ul>
<li><p><strong>Intro</strong>: what is and why we should use uniform convergence: (1-13, 1-14)</p>
<p>if there's only one function in F, we simply use law of large numbers, then we can prove convergence, but it is of no use.</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g95pm0mzxxj31760iqaeu.jpg"></p>
<p><strong>Example</strong>: when F contains more functions and here the case is when dataset are separated, empirical risk are sometimes not convergence to real risk. (1-13)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g95px5f6gjj317m0jgwip.jpg"></p>
<p><strong>Example</strong>: the GC theorem in function way (2-6)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g96450i51uj30o305pab5.jpg"></p></li>
<li><p><strong>Intro</strong>: using <strong>Lemma1</strong> in 1.2.3.2.3 to empirical functions (2-6)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123134507969.png"></p>
<p><strong>Usage</strong>: when we F specialize to indicators (2-6)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g964l986unj30ot0dm75n.jpg"></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g964li6gk5j30ny031aai.jpg"></p></li>
</ul>
<p>​</p>
<h5 id="vc-class-and-stuff">1.2.4.2.3. vc class and stuff</h5>
<p>Vc &amp; covering number are powerful tools to empirical learning.</p>
<p>######vc class (2-6,7, 3-5)</p>
<ul>
<li><p><strong>Def</strong>: vc class</p>
<p><strong>Usage</strong>: vc class , C is a collection of sets，X is the largest set, Vc is：given n points from X, if subsets of C cannot separate X into <span class="math inline">\(2 ^{n}\)</span> parts，then <span class="math inline">\(V ^{C}=n\)</span>, mind that we should go from n =1 to above, and see if we could find the x points that could be shattered by this set, if not we shop and set Vc = n. So basically, the larger Vc, the better C is in regards to shatting set X.</p>
<p>(2-6)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g964nsneufj30og02lt8y.jpg"></p>
<p><strong>Example</strong>: $V ^{C} = 2 $ since given <span class="math inline">\(x1=1, x2=2\)</span>, you can't separate them. (2-6)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g964pd0whnj30nv04ljrp.jpg"></p>
<p><strong>Example</strong>: half space (2-7)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g964vbn2zuj30o40210su.jpg"></p>
<p><strong>Example</strong>: subgraph (2-7)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g964vlz3hcj30og02gglt.jpg"></p>
<p>(3-5), nth shatter coeffcient is m(n), for all n &lt; Vc, we say F/H shatters {x1...xn}</p>
<p><img src="/2019/11/19/stats_theory/image-20191123231131104.png"></p>
<p><strong>Example</strong>: (3-5)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123232017857.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20191123232034601.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20191123232045980.png"></p>
<p><strong>Example</strong>: (3-5)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123232056701.png"></p>
<p><strong>Example</strong>: (3-5)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123232108760.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20191123232117823.png"></p></li>
<li><p><strong>Lemma</strong>: =&gt; bound Vc for a broad family of classes</p>
<p><img src="/2019/11/19/stats_theory/image-20191123232241905.png"></p></li>
<li><p><strong>Lemma</strong>: =&gt; Sauer's the relation ship between m(n) and Vc</p>
<p>(3-5)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123232312620.png"></p>
<p>(2-6)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g964pko8wij30ll03bjrf.jpg"></p>
<p><strong>Proof</strong>: (2-6)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g964r6bma5j30mv08w74u.jpg"></p>
<p><strong>Note</strong>: that we can use VC to see Pn, use m(n) to indicate the <span class="math inline">\(2 ^{n}\)</span> subsets of all, and we use lemma to get a upper bound of Pn0 (2-6)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123141515437.png"></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g964rjujfgj30o703a74c.jpg"></p>
<ul>
<li><p><strong>Corollary</strong>: (3-5)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123232332935.png"></p>
<ul>
<li><p><strong>Corollary</strong>: (3-5)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123232347065.png"></p></li>
<li><p><strong>Corollary</strong>: (3-5)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123232359432.png"></p></li>
</ul></li>
</ul></li>
<li><p><strong>Qua</strong>: operation (2-7)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g964sdwcymj30o3060dg7.jpg"></p>
<p>​</p></li>
</ul>
<h6 id="covering-number-2-7-2-8">1.2.4.2.3.1. covering number (2-7, 2-8)</h6>
<ul>
<li><p><strong>Def</strong>: covering number (a more powerful way than VC) (2-7)</p>
<p><strong>Usage</strong>: a covering number is the smallest number of functions that can approximate any f with metric Q in F. Apparently larger N1, the more complex class F, the more powerful it is in regards to classification. Here N1 means F is constrained within <span class="math inline">\(L ^{1}(Q)\)</span></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g964ys8qbkj30od02zgm9.jpg"></p>
<p><strong>Note</strong>: not unique s (2-7)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g964whyhyoj30n801j747.jpg"></p></li>
<li><p><strong>Theorem</strong>: =&gt; apporximation lemma, giving an upper bound for covering number of VC dimension(2-8)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123152755956.png"></p>
<p><strong>Usage</strong>: (2-8)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123152820198.png"></p></li>
</ul>
<h6 id="entropy">1.2.4.2.3.2. entropy</h6>
<ul>
<li><p><strong>Def</strong>: metric entropy (2-7)</p>
<p><strong>Usage</strong>: metric entropy is log N1.</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g964yzm3znj30og02fdg3.jpg"></p>
<p><strong>Example</strong>: (2-7)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123151349836.png"></p>
<p><strong>Proof</strong>: computing cover number for F. Using definition of covering number, constructing <span class="math inline">\(\hat{f}\)</span> for F (2-7)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123151358537.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20191123151407404.png"></p></li>
<li><p><strong>Def</strong>: totally bounded (2-7)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g964z6ewrbj30nh018aa2.jpg"></p></li>
<li><p><strong>Def</strong>: entropy with bracking (2-7)</p>
<p><strong>Usage</strong>: Np,B means the smallest of m pairs of functions that can approximate any f in F.</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g964zbzzkoj30oe047aaz.jpg"></p></li>
<li><p><strong>Qua</strong>: relation of metric &amp; brackeing entropy (2-7)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g964ziid51j30nx05daal.jpg"></p></li>
</ul>
<h6 id="vc-theory-for-sets3-5">1.2.4.2.3.3. vc theory for sets(3-5)</h6>
<ul>
<li><p><strong>Theorm</strong>: VC theory =&gt; convergence guarantees for an empirical risk minimizing classifier when the VC dimen- sion of the classifier set H was finite. (3-5)</p>
<p><img src="/2019/11/19/stats_theory/image-20191023091054642.png"></p>
<ul>
<li><p><strong>Corollary</strong> DKW</p>
<p><img src="/2019/11/19/stats_theory/image-20191031155626864.png"></p></li>
</ul></li>
<li><p><strong>Qua</strong>: operation.</p>
<p><img src="/2019/11/19/stats_theory/image-20191015161350400.png"></p></li>
</ul>
<h6 id="mono-layer-a-special-classifier3-5">1.2.4.2.3.4. mono layer (a special classifier)(3-5)</h6>
<ul>
<li><p><strong>Def</strong>: see page 8 of class_3 lecture 5</p>
<ul>
<li><p><strong>Theorm</strong>: =&gt; bound of expected shatter coefficient</p>
<p><img src="/2019/11/19/stats_theory/image-20191023091839892.png"></p>
<ul>
<li><p><strong>Corollary</strong>: =&gt; convergence</p>
<p><img src="/2019/11/19/stats_theory/image-20191023091951058.png"></p></li>
</ul></li>
</ul></li>
</ul>
<h5 id="p-glivenko-cantelli-for-class-f">1.2.4.2.4. P-Glivenko-Cantelli (for class F)</h5>
<p>This is where we finally arrived at, the generalized GC.</p>
<ul>
<li><p><strong>Def</strong>: P-Glivenko-Cantelli class(2-8)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123121333880.png"></p></li>
<li><p><strong>Def</strong>: envelop (2-8)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123121420040.png"></p>
<ul>
<li><p><strong>Theorem</strong>: =&gt; P-GC (2-8)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123121743529.png"></p>
<p><strong>Note</strong>: intuition for condition of metric entropy (2-8)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123121757398.png"></p>
<p><strong>Proof</strong>: way too long</p>
<p><strong>Example</strong>: (2-8)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123122147883.png"></p></li>
</ul></li>
</ul>
<h2 id="alternative-loss-functions">1.3. alternative loss functions</h2>
<h3 id="maximum-likelihood">1.3.1. maximum likelihood</h3>
<ul>
<li><p><strong>Def</strong>: maximum likelihood <span class="math inline">\(l(f(x_i),y_i)\)</span></p>
<p><strong>Usage</strong>: turn to an alternative minimization goal, only available when the output is conditional probability</p>
<p><img src="/2019/11/19/stats_theory/image-20191204000442489.png"></p></li>
</ul>
<h3 id="probably-approximately-correct-1-13-3-4">1.3.2. probably approximately correct (1-13, 3-4)</h3>
<ul>
<li><p><strong>Def</strong>: PAC (1-13)</p>
<p><strong>Usage</strong>: turn to an alternative goal.</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g95o9tuhy7j315m07awg7.jpg"></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g95o9zv0drj315y0560tu.jpg"></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g95oaciywjj316y072wfk.jpg"></p>
<p>​</p>
<ul>
<li><p><strong>Def</strong>: probability approximate corret condition (1-13)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g95nkp8b07j31660ekwhc.jpg"></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g95nlmbv8fj316k092jsx.jpg"></p></li>
</ul></li>
<li><p><strong>Qua</strong>: PAC insights (1-13)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g95o8uz3txj315u06qgna.jpg"></p></li>
<li><p><strong>Def</strong>: PAC learnable (1-13)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g95olftvxlj316s0dqacl.jpg"></p></li>
<li><p><strong>Def</strong>: PAC definition 2 ( 3-4)</p>
<p><img src="/2019/11/19/stats_theory/image-20191123181529769.png"></p></li>
</ul>
<h1 id="supervised-algorithms">2. supervised algorithms</h1>
<h2 id="decision-theory">2.1. decision theory</h2>
<ul>
<li><p><strong>Def</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200222195231293.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200222195242137.png"></p></li>
<li><p><strong>Note</strong>: model choosing -&gt; loss function choosing -&gt; optimization (training) -&gt; validation</p></li>
</ul>
<h3 id="model">2.1.1. model</h3>
<ul>
<li><p><strong>Def</strong>: model</p>
<p><img src="/2019/11/19/stats_theory/image-20200222195345836.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200222195405055.png"></p></li>
</ul>
<h4 id="generative-model">2.1.1.1. generative model</h4>
<ul>
<li><p><strong>Def</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200222204848422.png"></p>
<ul>
<li><p><strong>Qua</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200222204914733.png"></p></li>
</ul></li>
</ul>
<h4 id="discriminative-model">2.1.1.2. discriminative model</h4>
<ul>
<li><p><strong>Def</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200222204900679.png"></p>
<ul>
<li><p><strong>Qua</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200222204925098.png"></p></li>
</ul></li>
</ul>
<h3 id="loss-function">2.1.2. loss function</h3>
<ul>
<li><p><strong>Def</strong>: loss function</p>
<p><img src="/2019/11/19/stats_theory/image-20200222195546912.png"></p></li>
</ul>
<h4 id="e-f-loss">2.1.2.1. E f loss</h4>
<ul>
<li><p><strong>Def</strong>: E f loss function</p>
<p><img src="/2019/11/19/stats_theory/image-20200222195613974.png"></p>
<ul>
<li><strong>Note</strong>: goal of training is to make this small</li>
</ul></li>
</ul>
<h4 id="em-f-loss">2.1.2.2. Em f loss</h4>
<ul>
<li><p><strong>Def</strong>: Em f loss</p>
<p><img src="/2019/11/19/stats_theory/image-20200222200726895.png"></p>
<ul>
<li><strong>Note</strong>: E f loss can't be obtained</li>
</ul></li>
<li><p><strong>Def</strong>: regularization</p>
<p><img src="/2019/11/19/stats_theory/image-20200222204458611.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200222204505015.png"></p></li>
<li><p><strong>Def</strong>: Em-f*-loss</p>
<p><img src="/2019/11/19/stats_theory/image-20200222204714362.png"></p>
<ul>
<li><p><strong>Theorem</strong>: upper bound</p>
<p><img src="/2019/11/19/stats_theory/image-20200222204737130.png"></p></li>
</ul></li>
</ul>
<h4 id="max-likelihood">2.1.2.3. max likelihood</h4>
<ul>
<li><strong>Def</strong>: max likelihood</li>
</ul>
<h3 id="optimization-algorithm">2.1.3. optimization algorithm</h3>
<ul>
<li><p><strong>Def</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200222201320978.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200222201332835.png"></p></li>
</ul>
<h3 id="validation-criteria">2.1.4. validation criteria</h3>
<p>####TN &amp; TP</p>
<ul>
<li><p><strong>Usage</strong>: criteria for 2-classification</p></li>
<li><p><strong>Def</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200221225011848.png"></p></li>
<li><p><strong>Def</strong></p>
<p><img src="/2019/11/19/stats_theory/image-20200222205007587.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200221225028800.png"></p></li>
</ul>
<h4 id="accuracy">2.1.4.1. accuracy</h4>
<ul>
<li><p><strong>Usage</strong>: classification for &gt; 2</p></li>
<li><p><strong>Def</strong>;</p>
<p><img src="/2019/11/19/stats_theory/image-20200222201636430.png"></p></li>
</ul>
<h3 id="validation-improvement">2.1.5. validation improvement</h3>
<h4 id="validation-set">2.1.5.1. validation set</h4>
<ul>
<li><p><strong>Def</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200221225411456.png"></p>
<ul>
<li><p><strong>Qua</strong>: cons</p>
<p><img src="/2019/11/19/stats_theory/image-20200221225427236.png"></p></li>
</ul></li>
</ul>
<h4 id="leave-one-out-validation">2.1.5.2. leave one out validation</h4>
<ul>
<li><p><strong>Def</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200221225457278.png"></p>
<ul>
<li><p><strong>Qua</strong>: pro</p>
<p><img src="/2019/11/19/stats_theory/image-20200221225508366.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200221225530194.png"></p></li>
<li><p><strong>Qua</strong>: special for regression</p>
<p><img src="/2019/11/19/stats_theory/image-20200221225609417.png"></p></li>
</ul></li>
</ul>
<h4 id="leave-k-out-validation">2.1.5.3. leave k out validation</h4>
<ul>
<li><p><strong>Def</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200221230028280.png"></p>
<ul>
<li><p><strong>Qua</strong>: balance</p>
<p><img src="/2019/11/19/stats_theory/image-20200221230123177.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200221230130238.png"></p></li>
</ul></li>
</ul>
<h4 id="bootstrap">2.1.5.4. bootstrap</h4>
<ul>
<li><p><strong>Def</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200221230327053.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200221230335139.png"></p></li>
</ul>
<h3 id="high-dimension-problem">2.1.6. high dimension problem</h3>
<ul>
<li><p><strong>Def</strong>: high dimension data</p>
<p><img src="/2019/11/19/stats_theory/image-20200222011534422.png"></p></li>
<li><p><strong>Def</strong>: high dimension problem -&gt; regression</p>
<p><img src="/2019/11/19/stats_theory/image-20200222011616320.png"></p></li>
<li><p><strong>Def</strong>: problem2</p>
<p><img src="/2019/11/19/stats_theory/image-20200222011657225.png"></p></li>
<li><p><strong>Def</strong>: problem3</p>
<p><img src="/2019/11/19/stats_theory/image-20200222012240963.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200222012247357.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200222012255444.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200222012318683.png"></p></li>
</ul>
<h2 id="knn-algorithm">2.2. knn algorithm</h2>
<ul>
<li><p><strong>Usage</strong>: discriminative model</p></li>
<li><p><strong>Def</strong>: choose the most appeared class in x's neiborhood</p>
<p>$$ p(y=c_j|x) = I[c_j=<em>c </em>{x_i N_k(x)}^{} I(y_i=c_j)]</p>
<p>$$</p>
<p><img src="/2019/11/19/stats_theory/image-20200222220748859.png"></p>
<ul>
<li><p><strong>Note</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200222221743143.png"></p></li>
</ul></li>
</ul>
<h3 id="loss-function-1">2.2.1. loss function</h3>
<ul>
<li><p><strong>Def</strong>: 0/1 loss function</p>
<p><span class="math display">\[
L(f, x, y))=I[y \neq f(x)] =&gt; E(L(Y,f(X))) 
\]</span></p>
<p><img src="/Users/huangbenson/SteinsGate9.github.io/source/_posts/stats_theory/2020-02-25-20-07-18-image.png"></p>
<ul>
<li><strong>Note</strong>: how to reach <span class="math inline">\(f\)</span>: preset structur: neighbor voting</li>
<li>note: how to get res: intuition.</li>
<li><strong>Note</strong>: how to reach <span class="math inline">\(\hat{f}\)</span>: to minimize EmL, we find that if we use the result from dataset <span class="math inline">\(\sum_{x \in N_{k}(x)} I\left(y_{i}=c_{j}\right)\)</span> then it is equivalent to trainning EmL(f, X, Y).</li>
</ul></li>
</ul>
<h3 id="alternatives-improvements">2.2.2. alternatives &amp; improvements</h3>
<h4 id="distances">2.2.2.1. distances</h4>
<ul>
<li><p><strong>Note</strong>: different distances</p>
<p><img src="/2019/11/19/stats_theory/image-20200222221912680.png"></p></li>
</ul>
<h4 id="choose-of-k">2.2.2.2. choose of k</h4>
<ul>
<li><p><strong>Def</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200222222036084.png"><img src="/2019/11/19/stats_theory/image-20200222224220219.png"></p>
<h3 id="implementation">2.2.3. implementation</h3></li>
<li><p><strong>Def</strong>: kd tree</p>
<p><img src="/2019/11/19/stats_theory/image-20200222224319598.png"></p></li>
<li><p><strong>Def</strong>: how to build</p>
<p><img src="/2019/11/19/stats_theory/image-20200222224413100.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200222224501055.png"></p>
<ul>
<li><p><strong>Note</strong>: 1 layer =&gt; 1 dimension, 2 layer =&gt; 2 dimension, ...</p></li>
<li><p><strong>Note</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200222230839198.png"></p></li>
</ul></li>
<li><p><strong>Def</strong>: how to use kd tree</p>
<p><img src="/2019/11/19/stats_theory/image-20200222225618355.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200222225627831.png"></p></li>
</ul>
<h2 id="perceptron-algorithm">2.3. perceptron algorithm</h2>
<ul>
<li><p><strong>Usage</strong>: discriminative model</p></li>
<li><p><strong>Def</strong>: The perceptron algorithm performs pattern classification when F is the class of linear threshold functions.</p>
<p>$$ p(y=1|x) = I[f(x)=sign(w ^{T} x)&gt;0]</p>
<p>$$</p>
<p><img src="https://i.loli.net/2019/11/20/eJ6NBMAcbIR9mFO.png"></p></li>
<li><p><strong>Def</strong>: how to build</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g94uosvr98j30us09gt9p.jpg"></p>
<ul>
<li><p><strong>Note</strong>: figure of perceptron</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g94uoljm4lj30v00emta4.jpg"></p>
<ul>
<li><p><strong>Qua</strong>: One important feature of this algorithm is that it only uses inner products of the data points.</p>
<p>let, where <span class="math inline">\(n_i\)</span> is the time that <span class="math inline">\(x_i\)</span> gets updated.</p>
<p><span class="math display">\[
\alpha_i = n_i \eta
\]</span></p>
<p>so we have</p>
<p><span class="math display">\[
w_N = \sum_{n=1}^{N} \alpha_i y_i x_i \\
b_N = \sum_{n=1}^{N} \alpha_i y_i
\]</span></p>
<p>the prediction rule becomes</p>
<p><span class="math display">\[
\hat{y}_i = sign(w_N ^{T}x _{i}+b_N) = sign( \sum_{j}^{}a _{j}x _{j} ^{T} y_jx _{i}+b_N)
\]</span></p>
<p>updates becomes (i is one of the misclassified points)</p>
<p><span class="math display">\[
a _{i} ^{(t+1)} = a _{i} ^{(t)} + \eta
\]</span></p>
<p><img src="https://i.loli.net/2019/11/20/tgpQOvDowBsfxHN.png"></p>
<ul>
<li><p><strong>Note</strong>: from this quality we can see the inspiration of kernel, that we only need the inner product of xixj, with the quality of readiness of computing inner product after projection, we can use kernel to get a more complex model</p>
<p><img src="https://i.loli.net/2019/11/20/q68lQbYzNG9xjSs.png"></p></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<h3 id="loss-function-2">2.3.1. loss function</h3>
<ul>
<li><p><strong>Def</strong>:</p>
<p><span class="math display">\[
L(w,x,y) = -y(wx+b) I[f(x)y&lt;0] =&gt;-\sum_{x_{i} \in \mathbb{M}} y_{i}\left(w \cdot x_{i}+b\right)
\]</span></p>
<p><img src="/2019/11/19/stats_theory/image-20200222205804020.png"></p>
<ul>
<li><p><strong>Note</strong>: how to reach <span class="math inline">\(f\)</span>: preset structure: linear model</p></li>
<li><p>note: how to get res: intuition.</p></li>
<li><p><strong>Note</strong>: how to reach <span class="math inline">\(\hat{f}\)</span>: minimizing EmL: which is distances of mis classified nodes, we want them to classify correctly=&gt;loss=0. note that we don't use the normal loss functions. Training = partial gradient descent(point by point).</p></li>
</ul></li>
</ul>
<h3 id="optimization-algorithm-1">2.3.2. optimization algorithm</h3>
<ul>
<li><p><strong>Def</strong>:gradient descent</p>
<p><img src="/2019/11/19/stats_theory/image-20200222210035729.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200222213437099.png"></p></li>
</ul>
<h3 id="convergence">2.3.3. convergence</h3>
<ul>
<li><p><strong>Theorem</strong>: complexity of the algorithm to converge.</p>
<ul>
<li><p><strong>Usage</strong>: result gives a bound on the number of steps in terms of some properties of the data.</p>
<p><img src="https://i.loli.net/2019/11/20/Qpn3gXHzTPF2aZY.png"></p></li>
<li><p><strong>Note</strong>: perceptron complexity may not be linear, it is possible it do not terminate after a long time</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g94uo4jzd8j30uk030aak.jpg"></p></li>
</ul></li>
</ul>
<h3 id="evaluation">2.3.4. evaluation</h3>
<ul>
<li><p><strong>Lemma</strong>: linearly independent =&gt; seperatable data</p>
<p><img src="https://i.loli.net/2019/11/20/NcJWjmECne4zbg1.png"></p>
<p><strong>Proof</strong>:</p>
<p><img src="https://i.loli.net/2019/11/20/5goRmWdDwpyv8hK.png"></p>
<ul>
<li><p><strong>Theorem</strong> : The main theorem in this section shows that for any prediction rule (from the class of linear threshold functions), there is a probability distribution that is linearly separable, but for which the prediction rule does poorly when averaged over all training data sets.</p>
<p><strong>Usage</strong>: we can compute this lower bound, to estimate the risk for this threshold function.</p>
<p><img src="https://i.loli.net/2019/11/20/MtqP29VGAsvCU6B.png"></p>
<p><strong>Proof</strong>: too long</p>
<p><strong>Note</strong> 1: see this bound as minimax lower bound, meaning that whatever <span class="math inline">\(f _{n}\)</span> you choose, there exists some part of the training dataset that yield result with lower bound.</p>
<p><img src="https://i.loli.net/2019/11/20/VqbT2vfYJE9oSH3.png"></p>
<p><strong>Note</strong> 2: generalization using VC dimension.</p>
<p><img src="https://i.loli.net/2019/11/20/zdSptJbWMUu9DQm.png"></p></li>
</ul></li>
<li><p><strong>Theorem</strong>: If we restrict our attention to probability distributions that have a large margin, we can prove an upper bound for the risk of the perceptron algorithm:</p>
<p><img src="https://i.loli.net/2019/11/20/OzXTrZA4jmY9hVx.png"></p>
<p><strong>Proof</strong>:</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g94un7bhjsj30wc0byta5.jpg"></p></li>
<li><p><strong>Theorm</strong>: In the above theorem, we were maximizing over all probability distributions. If we restrict the maximization to be over those probability distributions having a large margin we can obtain a better lower bound:</p>
<p><img src="https://i.loli.net/2019/11/20/HR7Fxa9ZECuMVeS.png"></p>
<p><strong>Proof</strong>:</p>
<p><img src="https://i.loli.net/2019/11/20/h8gfUSourM4qYQw.png"></p></li>
</ul>
<h2 id="support-vector-machine">2.4. support vector machine</h2>
<ul>
<li><p><strong>Usage</strong>: special form of perceptron(same structure) where distance is maximized.</p></li>
<li><p><strong>Def</strong>: function distance1</p>
<p><img src="/2019/11/19/stats_theory/image-20200224175649655.png"></p></li>
<li><p><strong>Def</strong>: geometry distance</p>
<p><img src="/2019/11/19/stats_theory/image-20200224175924749.png"></p>
<ul>
<li><p><strong>Qua</strong>: relation ship with distance 1, we know that geometry distance is not affected by w alone.</p>
<p><img src="/2019/11/19/stats_theory/image-20200224175943803.png"></p></li>
</ul></li>
<li><p><strong>Def</strong>: support vector machine model</p>
<p>$$ p(y=1|x) = I[f(x)=sign(w ^{T} x)&gt;0]</p>
<p>$$</p>
<p><img src="https://s2.ax1x.com/2019/11/20/MWp8JJ.png"></p>
<ul>
<li><strong>Note</strong>:</li>
</ul>
<p><img src="https://s2.ax1x.com/2019/11/20/MWpAiQ.png" alt="MWpAiQ.png" style="zoom:89%;"></p></li>
</ul>
<h3 id="loss-fucntion">2.4.1. loss fucntion</h3>
<ul>
<li><p><strong>Def</strong>: loss function for soft C, if hardmargin(basic form), lambda = 1/infinity</p>
<p><span class="math display">\[
L(w,x,y) = ||w||^2+[1-y(w*x+b)]_+ =&gt;\sum_{i=1}^{N}\left[1-y_{i}\left(w \cdot x_{i}+b\right)\right]_{+}+\lambda\|w\|^{2}
\]</span></p>
<p><img src="/2019/11/19/stats_theory/image-20200224185638711.png"></p>
<ul>
<li>note: that [1-x]+ vs I(x)x&gt;0. 这里I[x]可以用于可分，因为它最少就是0，是0的时候就是可分的。同理我们也可以用硬间隔把[1-x]+前面的系数调成正无穷，那么效果是和I[x]一样的。只不过我们多加了||w||这项，这项是为了最大化间隔。</li>
<li>note: how to get f: preset, linear model</li>
<li>note: how to get res: intuition.</li>
<li>note: how to get <span class="math inline">\(\hat{f}\)</span> : by minimizing EmL: mimizing not only the misclassified distance(hinge loss), but also the parameter to make distances as large as possible, training = optimization(IIS. newton).</li>
</ul></li>
</ul>
<h3 id="optimization">2.4.2. optimization</h3>
<h4 id="basic">2.4.2.1. basic</h4>
<ul>
<li><p><strong>Qua</strong>: basic transformations</p>
<p>trans: other form <span class="math inline">\(\left \| w\right \|=1\)</span></p>
<figure>
<img src="https://s2.ax1x.com/2019/11/20/MWpfw8.png" alt="MWpfw8.png"><figcaption>MWpfw8.png</figcaption>
</figure>
<p>trans: other form <span class="math inline">\(\left \| w\right \|=1/ \gamma\)</span></p>
<figure>
<img src="https://s2.ax1x.com/2019/11/20/MWpOmV.png" alt="MWpOmV.png"><figcaption>MWpOmV.png</figcaption>
</figure>
<p>overall</p>
<p><img src="/2019/11/19/stats_theory/image-20200224180800454.png"></p></li>
</ul>
<h4 id="dual">2.4.2.2. dual</h4>
<ul>
<li><p><strong>Def</strong>:</p>
<p>lagrange function</p>
<p><img src="/2019/11/19/stats_theory/image-20200224182038092.png"></p>
<p>goal: max min</p>
<p><img src="/2019/11/19/stats_theory/image-20200224182053693.png"></p>
<p>get lagrange dual problem ( solve the min problem)</p>
<p><img src="/2019/11/19/stats_theory/image-20200224182127376.png"></p>
<p>final: solve lagrange dual (max problem)</p>
<p><img src="/2019/11/19/stats_theory/image-20200224182207475.png"></p>
<p>recall slackenss for dual problem, this show that the those with ai(penalty loss) <span class="math inline">\(\neq\)</span> 0, are exactly those in the margins. 算法解只由支持向量决定。</p>
<p><img src="https://i.loli.net/2019/11/20/SwgJqitobc2Xvku.png"></p>
<p>recall the kernel stuff, we can use kernel to efficientlly solve this, for more kernel stuff turn to 2.2.2</p>
<p><img src="https://i.loli.net/2019/11/20/IrRWNKDMTS9mn6l.png"></p>
<p>overall</p>
<p><img src="/2019/11/19/stats_theory/image-20200224181234861.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200224181252809.png"></p></li>
</ul>
<h3 id="alternatives-optimization">2.4.3. alternatives &amp; optimization</h3>
<h4 id="kernel-svm">2.4.3.1. kernel SVM</h4>
<ul>
<li><p><strong>Def</strong>: dual problem &amp; kernel function</p>
<p>dual problem (only inner products determines solution) <img src="https://i.loli.net/2019/11/20/Jg5KwYC4clsp1LO.png"></p></li>
<li><p><strong>Qua</strong>: kernel function can reduce complexity of dual problem.( further discussion can be find in linera algebra)</p>
<p><strong>Usage</strong>: 由此看出，即使没有kernel，只要找到映射关系，我们仍然可以实现低维空间映射到高维空间，进而通过高维向量内积运算来寻找最佳分割超平面。但是，kernel的引入，降低了向量内积运算的计算复杂度。无论我们映射到几维空间，我们只要把原有维度代入核函数，就能等价于高维上的内积运算。极端的情况下，特征空间映射到了无穷维度空间，如果没有核函数，根本无法计算出映射后的向量内积。</p>
<p>（映射到新的特征空间后的内积=原空间核函数）</p>
<p>https://blog.csdn.net/zhangjun2915/article/details/79261368</p>
<p>k(xi, xj) is the kernel function of inner product of xi, xj</p></li>
<li><p><strong>Def</strong>: kernel function</p>
<p><img src="/2019/11/19/stats_theory/image-20200224222158590.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200224222211246.png"></p>
<ul>
<li><p><strong>Note</strong>: k = HxH-&gt;R, we project nodes to higher dimension and do SVM in higher dimension, the results are got by kernel inner project.</p></li>
<li><p><strong>Example</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200224225958578.png"></p></li>
<li><p><strong>Example</strong>: polynomial kernel</p>
<p><img src="/2019/11/19/stats_theory/image-20200224222037543.png"></p></li>
<li><p><strong>Example</strong>: gaussian kernel</p>
<p><img src="https://i.loli.net/2019/11/20/H13oW6M2znwQlKY.png"></p></li>
<li><p><strong>Theorem</strong>: what kind of K is kernel(can be split</p>
<p><img src="/2019/11/19/stats_theory/image-20200224233536888.png"></p></li>
</ul></li>
<li><p><strong>Def</strong>: f is a function space : R-&gt;R</p>
<p><img src="/2019/11/19/stats_theory/image-20200224231301936.png"></p>
<ul>
<li><p><strong>Def</strong>: inner product on S: SxS-&gt;R</p>
<p><img src="/2019/11/19/stats_theory/image-20200224231520882.png"></p></li>
<li><p><strong>Qua</strong>: the definition of f and innerproduct makes S a RKHS. S is a space (<span class="math inline">\(\phi\)</span>, K, f, <em>), where f &amp; </em> are self-defined function &amp; inner product. note that this quality does not come from K or <span class="math inline">\(\phi\)</span>, K could be any thing, RKHS comes from (f, *).</p>
<p><img src="/2019/11/19/stats_theory/image-20200224232457378.png"> <img src="/2019/11/19/stats_theory/image-20200224232511561.png"></p></li>
</ul></li>
<li><p>def:</p>
<p><img src="/2019/11/19/stats_theory/image-20200224235941414.png"></p></li>
</ul>
<h4 id="primitive-svm">2.4.3.2. primitive SVM</h4>
<ul>
<li><p><strong>Usage</strong>: a more natural way</p></li>
<li><p><strong>Def</strong>: primal SVM,</p>
<p><img src="https://i.loli.net/2019/11/20/ksrwUoluGiF7jMI.png"></p>
<p>lagrangian functiion</p>
<p><img src="https://i.loli.net/2019/11/20/eJiRtwo5M9AEkDN.png"></p>
<p>getting lagranian dual function</p>
<p><img src="https://i.loli.net/2019/11/20/oYLf9tT3VOjEwUW.png"></p>
<p>lagrangain dual function</p>
<p><img src="https://i.loli.net/2019/11/20/IyrRtMfzo4mJelU.png"></p>
<p>dual problem</p>
<p><img src="https://i.loli.net/2019/11/20/Wbrua1oidJILVPp.png"></p>
<p>this problem can actually be solved by hand: first solve <span class="math inline">\(\beta = \sqrt{1/4 \left \| \sum_{i}^{}\lambda _{i}y _{i } x _{i}\right \|}\)</span>, then we get the following</p>
<p><img src="https://i.loli.net/2019/11/20/8CZfKqxytmOoYih.png"></p>
<p>interpreting results</p>
<p><img src="https://i.loli.net/2019/11/20/WUvX9rTzie2xRn4.png"></p></li>
</ul>
<h4 id="soft-c-svm">2.4.3.3. soft C-SVM</h4>
<ul>
<li><p><strong>Usage</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200224184216241.png"></p></li>
<li><p><strong>Def</strong>: soft SVM</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g94kd7xe38j30sc024jrf.jpg"></p></li>
<li><p><strong>Qua</strong>: basic transform</p>
<p>trans: change |x&lt;1| to (1-x)+ , basiclly is the same .</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g94kdj455cj30ok02q3ym.jpg"></p>
<p>trans: slack variable</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g94kdp69btj30rs046aac.jpg"></p>
<p><img src="/2019/11/19/stats_theory/image-20200224184318763.png"></p>
<p>lagranrian function =&gt; dual lagranian function (solve min)</p>
<p><img src="/2019/11/19/stats_theory/image-20200224184511108.png"></p>
<p>dual problem (solve max)</p>
<p><img src="/2019/11/19/stats_theory/image-20200224184604328.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200224184619430.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200224184547938.png"></p>
<p>choose c</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g94l6l7zfcj316e0380tf.jpg"></p>
<p>overall <img src="/2019/11/19/stats_theory/image-20200224184845366.png"></p>
<ul>
<li><p><strong>Note</strong>: photo of soft support</p>
<p><img src="/2019/11/19/stats_theory/image-20200224184916504.png"></p></li>
</ul></li>
<li><p><strong>Qua</strong>: interpreting in another way( loss function + regularization)</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g94llq6xidj315w03ct92.jpg"></p></li>
</ul>
<h4 id="soft--svm">2.4.3.4. soft $ $-SVM</h4>
<ul>
<li><p><strong>Def</strong>: a more interpretable way, since <span class="math inline">\(\nu\)</span> is more easy to set.</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g94lf89bzoj314a05gwey.jpg"></p>
<ul>
<li><p><strong>Qua</strong>: basic transition</p>
<p>slack variables</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g94lfwvlcxj315607y0tg.jpg"></p>
<p>lagrange dual function</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g94lgbmkh1j30wi03oglu.jpg"></p>
<p>dual problem</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g94lgoido3j30y208oq3v.jpg"></p>
<p>insights for slackness</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g94lhpmmmmj316804wt9z.jpg"></p></li>
<li><p><strong>Qua</strong>: interpretation of <span class="math inline">\(\nu\)</span> : we can interpret <span class="math inline">\(nu\)</span> as being approximately the fraction of data that are margin errors.</p>
<p>[](/Users/huangbenson/Library/Application Support/typora-stats_theory(https://tva1.sinaimg.cn/large/006y8mN6ly1g94litz5uxj316604g3zx.jpg)</p></li>
</ul></li>
<li><p><strong>Qua</strong>: <span class="math inline">\(\nu\)</span> and C SVM are the same</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g94ljbz53fj316o02gt99.jpg"></p>
<p><strong>Proof</strong>:</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g94ljika20j317a0lon06.jpg"></p></li>
</ul>
<h4 id="sensitive-svm-1-10">2.4.3.5. $ $-sensitive SVM (1-10)</h4>
<ul>
<li><p><strong>Def</strong>: a relaxation of loss function</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g95jkwn25kj31d60hajuf.jpg"></p></li>
</ul>
<h3 id="convergence-1">2.4.4. convergence</h3>
<ul>
<li><p><strong>Theorem</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200224180955389.png"></p></li>
</ul>
<h3 id="relationship-with-logistic-regression">2.4.5. relationship with logistic regression</h3>
<ul>
<li><p><strong>Def</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200222013732650.png"></p>
<ul>
<li><p><strong>Note</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200222013803270.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200222013819229.png"></p></li>
</ul></li>
</ul>
<h2 id="discriminant-algorithm">2.5. discriminant algorithm</h2>
<h3 id="lda-by-distance">2.5.1. LDA: by distance</h3>
<ul>
<li><p><strong>Def</strong>: distance</p>
<p><span class="math display">\[
p(y=m|x) = I[d(x,G_m)=min_m]
\]</span></p>
<p><img src="/2019/11/19/stats_theory/image-20191201002514743.png"></p></li>
<li><p><strong>Algorithm</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20191201002702582.png"></p>
<ul>
<li><p>note: f preset structrue, minimize distance</p></li>
<li><p>note: f ha comes from computing parameter of dataset=&gt;distance</p></li>
</ul></li>
</ul>
<h4 id="if-sum1-sum-2">2.5.1.1. if \(\sum1 = \sum 2\)</h4>
<ul>
<li><p><strong>Algorithm</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20191201002926764.png"></p></li>
</ul>
<h4 id="if-sum1-neq-sum2">2.5.1.2. if \(\sum1 \neq \sum2\)</h4>
<ul>
<li><p><strong>Algorithm</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20191201003036044.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20191201003041939.png"></p></li>
</ul>
<h4 id="if-more-than-1">2.5.1.3. if more than 1</h4>
<ul>
<li><p><strong>Algorithm</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20191201003332732.png"></p></li>
</ul>
<h3 id="qda-by-general-distance">2.5.2. QDA: by general distance</h3>
<ul>
<li><p><strong>Algorithm</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20191201003459205.png"></p></li>
</ul>
<h3 id="by-max-posterior-estimate">2.5.3. by max posterior (estimate)</h3>
<ul>
<li><p><strong>Algorithm</strong>: not similar to logistic, logstic is a distribution in whole &amp; linear model, not using full probability expansion. This is similar to bayes =&gt; except this is exp normal model, bayes is discrete.</p>
<p><img src="/2019/11/19/stats_theory/image-20191201003651075.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20191201003734364.png"></p></li>
</ul>
<h3 id="by-bayes">2.5.4. by bayes</h3>
<ul>
<li><p><strong>Def</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20191201003748527.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20191201003923496.png"></p></li>
<li><p><strong>Def</strong>: miss classification loss</p>
<p><img src="/2019/11/19/stats_theory/image-20191201003837128.png"></p></li>
<li><p><strong>Def</strong>: total miss classification loss</p>
<p><img src="/2019/11/19/stats_theory/image-20191201003912925.png"></p></li>
<li><p><strong>Theorem</strong>: the main theorem of bayes classification</p>
<p><img src="/2019/11/19/stats_theory/image-20191201003941978.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20191201003947318.png"></p>
<p><strong>Corollary</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20191201004019825.png"></p></li>
</ul>
<h4 id="if-sum-all-the-same-gaussian">2.5.4.1. if \(\sum\) all the same (gaussian)</h4>
<ul>
<li><p><strong>Algorithm</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20191201004151416.png"></p></li>
</ul>
<h4 id="if-not-all-the-same-gaussian">2.5.4.2. if not all the same (gaussian)</h4>
<h3 id="by-fisher">2.5.5. by Fisher</h3>
<ul>
<li><p><strong>Def</strong>:</p>
<p><strong>Usage</strong>: adding projection procedure to separate the data.</p>
<p><img src="/2019/11/19/stats_theory/image-20191201004524088.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20191201004602624.png"></p></li>
<li><p><strong>Theorem</strong>: the best direction</p>
<p><img src="/2019/11/19/stats_theory/image-20191201004933939.png"></p></li>
</ul>
<h4 id="r1">2.5.5.1. r=1</h4>
<h5 id="k2">2.5.5.1.1. k=2</h5>
<ul>
<li><p><strong>Algorithm</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20191201005049589.png"></p></li>
</ul>
<h4 id="r">2.5.5.2. r=?</h4>
<ul>
<li><p><strong>Algorithm</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20191201005108232.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20191201005102049.png"></p></li>
</ul>
<h2 id="bayes-algorithm">2.6. bayes algorithm</h2>
<ul>
<li><p><strong>Usage</strong>: generative model</p></li>
<li><p><strong>Def</strong>:</p>
<p><span class="math display">\[
P\left(Y=c_{k} | X=x\right)=\frac{P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)}{\sum_{k} P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)}, \quad k=1,2, \cdots, K
\]</span></p>
<p><img src="/2019/11/19/stats_theory/image-20200222233335780.png"></p>
<ul>
<li><p><strong>Note</strong>: we have to assume that X is independent in every dimension（every dimension is a feature)</p>
<p><img src="/2019/11/19/stats_theory/image-20200222233348331.png"></p></li>
</ul></li>
<li><p><strong>Def</strong>: using kernel to interpretate</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g95jk8garjj31ca0ma79u.jpg"></p></li>
</ul>
<h3 id="loss-function-3">2.6.1. loss function</h3>
<ul>
<li><p><strong>Def</strong>: 0/1 loss</p>
<p><span class="math display">\[
L(f,x,y)=I[{y \neq f(x)}]=&gt;E(L(Y,f(X)))
\]</span></p>
<p><img src="/Users/huangbenson/SteinsGate9.github.io/source/_posts/stats_theory/2020-02-25-19-57-45-image.png"><img src="/2019/11/19/stats_theory/image-20200223132344665.png"></p>
<ul>
<li><strong>Note</strong>: how to get f: bayes does not have a presumed structrue like perceptron / knn, it(f) derives from the basic rule of conditional probability</li>
<li>note: how to get res: it derives from minimizing EL with 0/1 loss function. Actually every algorithm that uses argmax post probability is minimizing EL.</li>
<li><strong>Note</strong>: how to get <span class="math inline">\(\hat{f}\)</span>: instead of minimizing EmL, we use max likelihood to get $ $.</li>
</ul></li>
</ul>
<h3 id="optimization-algorithm-2">2.6.2. optimization algorithm</h3>
<ul>
<li><p><strong>Def</strong>: max likelihood</p>
<p><img src="/2019/11/19/stats_theory/image-20200223005339040.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200223135926230.png"></p>
<ul>
<li><p><strong>Note</strong>: that here we assume discrete feature of X, and the derivation is here : https://blog.csdn.net/weixin_41575207/article/details/81709379</p></li>
<li><p><strong>Note</strong>: example</p>
<p><img src="/2019/11/19/stats_theory/image-20200223140003765.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200223140009293.png"></p></li>
</ul></li>
</ul>
<h3 id="alternatives-improvements-1">2.6.3. alternatives &amp; improvements</h3>
<h4 id="laplace-smooth">2.6.3.1. laplace smooth</h4>
<ul>
<li><p><strong>Def</strong>: laplace smooth</p>
<p><img src="/2019/11/19/stats_theory/image-20200223143507596.png"></p>
<ul>
<li><p><strong>Note</strong>: example</p>
<p><img src="/2019/11/19/stats_theory/image-20200223143520591.png"></p></li>
</ul></li>
</ul>
<h2 id="logistic-regression-1-13">2.7. logistic regression (1-13)</h2>
<ul>
<li><p><strong>Def</strong>: distribution</p>
<p><img src="/2019/11/19/stats_theory/image-20200223175311247.png"></p>
<ul>
<li><p><strong>Note</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200223175326033.png"></p></li>
</ul></li>
<li><p><strong>Def</strong>: logistic regression model</p>
<p><span class="math display">\[
P(Y=1 | x)=\frac{\exp (w \cdot x+b)}{1+\exp (w \cdot x+b)}
\]</span></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g95mj6tnn0j318o0640w1.jpg"></p>
<p><img src="/2019/11/19/stats_theory/image-20200221222911920.png"></p></li>
<li><p><strong>Def</strong>: logit = log(p/1-p)</p>
<p><img src="/2019/11/19/stats_theory/image-20200221223315528.png"></p></li>
</ul>
<h3 id="loss-function-4">2.7.1. loss function</h3>
<ul>
<li><p><strong>Def</strong>: loss function = - log(likelihood)</p>
<p><span class="math display">\[
L(f,x,y) = -log(
\prod_{i=1}^{N}\left[\pi\left(x_{i}\right)\right]^{y_{i}}\left[1-\pi\left(x_{i}\right)\right]^{1-y_{i}}
)
\]</span></p>
<p><img src="/2019/11/19/stats_theory/image-20200223180704186.png"></p>
<ul>
<li><p><strong>Note</strong>: likelihood function is p(y|x) not p(x,y) since we maximize w, adding p(x) makes no difference. In one word, we save all parts containing w.</p></li>
<li><p><strong>Note</strong>: how to get f: preset structure: logistic distribution</p></li>
<li><p>note: how to get res: same as bayes?</p></li>
<li><p><strong>Note</strong>: how to get <span class="math inline">\(\hat{f}\)</span>: maximize likelihood/ minimize EmL to get $ $</p></li>
</ul></li>
</ul>
<h2 id="max-entropy-algorithm">2.8. max entropy algorithm</h2>
<ul>
<li><p><strong>Def</strong>: special function</p>
<p><img src="/2019/11/19/stats_theory/image-20200223190404515.png"></p>
<ul>
<li><p><strong>Qua</strong>: E</p>
<p><img src="/2019/11/19/stats_theory/image-20200223190413981.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200223190508260.png"></p></li>
<li><p><strong>Qua</strong>: E is used to test if p(y|x) is empirical p(y|x)(if p(y|x) suits the dataset)</p>
<p><img src="/2019/11/19/stats_theory/image-20200223190854106.png"></p></li>
</ul></li>
<li><p><strong>Def</strong>: model</p>
<p><span class="math display">\[
p(y|x) = f(x,y) \text{ 代求}
\]</span></p>
<ul>
<li><strong>Note</strong>: that this model does not have any form of structure.</li>
</ul></li>
</ul>
<h3 id="loss-function-5">2.8.1. loss function</h3>
<ul>
<li><p><strong>Def</strong>: loss function</p>
<p><span class="math display">\[
L(P,x,y) = -\sum_{x, y} \tilde{P}(x) P(y | x) \log P(y | x)
\]</span></p>
<p><img src="/2019/11/19/stats_theory/image-20200223191622978.png"></p>
<ul>
<li><p><strong>Note</strong>: note that this model maximize H(P) = make y as smooth &amp; normal &amp; equal as possible</p></li>
<li><p><strong>Note</strong>: how to get f: no preset form!!</p></li>
<li><p>note: how to get res: same as bayes.</p></li>
<li><p><strong>Note</strong>: how to get $ $ : use empirical conditional entropy and solve optimization problem(training), traning = optimization.</p></li>
</ul></li>
</ul>
<h3 id="optimization-1">2.8.2. optimization</h3>
<h4 id="dual-problem">2.8.2.1. dual problem</h4>
<ul>
<li><p><strong>Def</strong>: this is the result of max entropy problem（p,w relationship, w still needs to be solved). Notice that this looks exactly like logistic regression.</p>
<p><span class="math display">\[
\begin{array}{c}{P_{w}(y | x)=\frac{1}{Z_{w}(x)} \exp \left(\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)} \\ {Z_{w}(x)=\sum_{y} \exp \left(\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)}\end{array}
\]</span></p>
<ul>
<li><p>the original maximization problem</p>
<p><img src="/2019/11/19/stats_theory/image-20200223200711546.png"></p></li>
<li><p>the minimization problem</p>
<p><img src="/2019/11/19/stats_theory/image-20200223200749655.png"></p></li>
<li><p>dual problem, here we go from solving P =&gt; solving w =&gt; 把w带回P，得到P(w)，其实就是本来直接可以解P但是不好解，现在换个对偶问题解w，然后带回p和w的关系式得到P。这里的x,y指的是可能取值而不是变量。而且求导是对某一个x，y求导，这里写的是对所有x，y对求导。</p>
<p><img src="/2019/11/19/stats_theory/image-20200223200839235.png"></p></li>
</ul></li>
<li><p>solve min problem by first order rule, get p,w relationship. note that we get derivation with each pair of x &amp; y, it is equivalent to getting with 1 pair.</p>
<p><img src="/2019/11/19/stats_theory/image-20200223201104710.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200223201114398.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200223201208033.png"></p></li>
<li><p>solve the outside max problem, get w.</p>
<p><img src="/2019/11/19/stats_theory/image-20200223201724406.png"></p></li>
<li><p>get back p(w)</p>
<p><img src="/2019/11/19/stats_theory/image-20200223201738461.png"></p></li>
</ul>
<h4 id="max-likelihood-1">2.8.2.2. max likelihood</h4>
<ul>
<li><p><strong>Usage</strong>: max likelihood = max entropy</p>
<p><img src="/2019/11/19/stats_theory/image-20200223205257326.png"></p></li>
<li><p><strong>Def</strong>:</p>
<ul>
<li><p>start from likelihood function，why in this form: https://blog.csdn.net/wkebj/article/details/77965714</p>
<p><img src="/2019/11/19/stats_theory/image-20200223210113859.png"></p></li>
<li><p>likelihood + p,w relationship =&gt; a function we need to maximize f(x,y,w)</p>
<p><img src="/2019/11/19/stats_theory/image-20200223213440569.png"></p></li>
<li><p>dual problem =&gt; a function we need to maximize f(x,y,w)</p>
<p><img src="/2019/11/19/stats_theory/image-20200223213602551.png"></p></li>
</ul></li>
</ul>
<h3 id="iis">2.8.3. IIS</h3>
<ul>
<li><p><strong>Usage</strong>: similar to gradient descent</p>
<p><img src="/2019/11/19/stats_theory/image-20200224172850709.png"></p></li>
<li><p><strong>Def</strong>:</p></li>
</ul>
<h2 id="ensemble-algorithm">2.9. ensemble algorithm</h2>
<ul>
<li><p><strong>Def</strong>: ensemble methods</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g95jox8hk9j311i0u042p.jpg"></p></li>
</ul>
<h3 id="boost">2.9.1. boost</h3>
<ul>
<li><p><strong>Def</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200222142426917.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200222142444145.png"></p></li>
</ul>
<h3 id="adaboost-algorithm">2.9.2. adaboost algorithm</h3>
<ul>
<li><p><strong>Usage</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200225003311056.png"></p></li>
<li><p><strong>Def</strong>: adaboost algorithm</p>
<p><img src="/2019/11/19/stats_theory/image-20200225003414890.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200225003422240.png"></p>
<ul>
<li><p><strong>Note</strong>: explain why we do this way =&gt; update weight + training.</p>
<p><img src="/2019/11/19/stats_theory/image-20200225003719069.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200225003724934.png"></p></li>
</ul></li>
</ul>
<h4 id="loss-function-6">2.9.2.1. loss function</h4>
<ul>
<li><p><strong>Def</strong>: general additive algorithm</p>
<p><img src="/2019/11/19/stats_theory/image-20200225011659794.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200225011705184.png"></p></li>
<li><p><strong>Def</strong>: loss function = exp</p>
<p><span class="math display">\[
L(f,x,y) = exp(-yf(x)) =&gt;\sum_{i=1}^{N} \bar{w}_{m i} \exp \left[-y_{i} \alpha G\left(x_{i}\right)\right]
\]</span></p>
<p><img src="/2019/11/19/stats_theory/image-20200225011723744.png"></p>
<ul>
<li><p><strong>Note</strong>: how do we get f: from equal distribution (no preset structure)</p></li>
<li><p>note: how to get res: intuition.</p></li>
<li><p><strong>Note</strong>: how do we get <span class="math inline">\(\hat{f}\)</span>: by optimizing exp loss function each step.（这个损失是整体的，也就是大目标就是为了让指数损失最小）</p>
<p><img src="/2019/11/19/stats_theory/image-20200225112656786.png"></p></li>
</ul></li>
<li><p><strong>Theorem</strong>: use gradient decent to explain adaboost, <strong>THIS IS THE SAME IDEA AS ABOVE</strong></p>
<p><strong>Usage</strong>: iteratively solving adaboost is equivalent to minimizing the true loss function of <span class="math inline">\(e ^{-YF _{T}(x)}\)</span></p>
<figure>
<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g95jsuxibxj316205wgml.jpg" alt="img"><figcaption>img</figcaption>
</figure>
<p><strong>Proof</strong>:</p>
<figure>
<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g95lgbda3bj319u0h6dj1.jpg" alt="img"><figcaption>img</figcaption>
</figure>
<p><strong>Note</strong>: loss function: For every t from 1 to T, we can expand the E(exp(-YX)) thing, and we can see that adaboost is minimizing Eexp(YFT(X)) at every step, thus the we are using step-wise gradient decent to optimize the Eexp(-YX).</p>
<p>The whole picture: So this is the idea of adaboost, we choose fT to minimize eT as a step-wise gradient descent, then we find out we can see adaboost as E(-YX) in whole, so we choose <span class="math inline">\(\alpha_T\)</span> to make that happen.</p>
<figure>
<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g95ldb2stkj31ac05g40g.jpg" alt="img"><figcaption>img</figcaption>
</figure>
<figure>
<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g95m3zu4mkj316c054mxx.jpg" alt="img"><figcaption>img</figcaption>
</figure></li>
</ul>
<h4 id="optimization-2">2.9.2.2. optimization</h4>
<ul>
<li><strong>Def</strong>: gradient descent .</li>
</ul>
<h4 id="convergence-2">2.9.2.3. convergence</h4>
<ul>
<li><p><strong>Theorem</strong>: there exist upper bound for loss function of each step, if database conditions are met.</p>
<figure>
<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g95l4x2rifj316a05ugml.jpg" alt="img"><figcaption>img</figcaption>
</figure>
<p><strong>Proof</strong>:</p>
<figure>
<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g95l513c30j315i0f0q57.jpg" alt="img"><figcaption>img</figcaption>
</figure></li>
<li><p><strong>Theorem</strong>: for all classification upper bound for loss</p>
<p><img src="/2019/11/19/stats_theory/image-20200225003955539.png"></p>
<ul>
<li><p><strong>Usage</strong>: the theory merely use update information=&gt;tells us if we update weights in this way, how do we train Gm</p>
<p><img src="/2019/11/19/stats_theory/image-20200225004947206.png"></p></li>
<li><p><strong>Proof</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200225004925903.png"></p></li>
</ul></li>
<li><p><strong>Theorem</strong>: for 2-class</p>
<p><img src="/2019/11/19/stats_theory/image-20200225005003342.png"></p>
<ul>
<li><p><strong>Proof</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200225005020613.png"></p></li>
</ul></li>
<li><p><strong>Corollary</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200225005111311.png"></p>
<ul>
<li><p><strong>Usage</strong>: if we update this way + training each round constrain <span class="math inline">\(e_m\)</span> to a constant, then algorithm converges in exp rate.</p>
<p><img src="/2019/11/19/stats_theory/image-20200225005121597.png"></p></li>
</ul></li>
</ul>
<h4 id="alternatives-improvement">2.9.2.4. alternatives &amp; improvement</h4>
<h5 id="generalized">2.9.2.4.1. generalized</h5>
<ul>
<li><p><strong>Def</strong>: generalized adaboost:</p>
<p><strong>Usage</strong>: in adaboost, we use exp(-YX) as loss, here we can have different options and still implement the step-wise adaboost core idea: we see J(F) as the loss, F is the variable, and each step we take <span class="math inline">\(F+\alpha _{t}f _{t}\)</span></p>
<figure>
<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g95jtgz619j31880kmad7.jpg" alt="img"><figcaption>img</figcaption>
</figure>
<p>recall: (convex optimization)</p>
<figure>
<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g95luzezzcj31b00d0dml.jpg" alt="img"><figcaption>img</figcaption>
</figure>
<p><strong>Algorithm</strong>:</p>
<figure>
<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g95jtonm5mj316u0by0vb.jpg" alt="img"><figcaption>img</figcaption>
</figure></li>
</ul>
<h5 id="boosting-tree">2.9.2.4.2. boosting tree</h5>
<ul>
<li><p><strong>Def</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200225013323947.png"></p></li>
</ul>
<h5 id="gradient-boosting">2.9.2.4.3. gradient boosting</h5>
<ul>
<li><p><strong>Def</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200225013259752.png"></p>
<ul>
<li><strong>Note</strong>: we can imagine gradient descent is using one model and doing descent by updating weight, and gradient boosting is using gradient descent by adding new model. <span class="math inline">\(f(x) = w1x+w2x+w3x\)</span> vs <span class="math inline">\(f(x) = f1(x) +f2(x) +f3(x)\)</span> , key point here is that f1(x) additive model is more flexible and differs from node to node even they share the same growing rate at same point(their derivative is the same).</li>
</ul></li>
</ul>
<h4 id="relationship-with-logistic-regression-1">2.9.2.5. relationship with logistic regression</h4>
<ul>
<li><p><strong>Qua</strong>: similarity to logistic regression (1-13)</p>
<p>using taylor expansion we can see that the first elements are the same (<span class="math inline">\(ln(1+e ^{-2a})\)</span>+1-ln2 &amp; e(-a))</p>
<figure>
<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g95mln63k6j316407o3zu.jpg" alt="img"><figcaption>img</figcaption>
</figure>
<p>e(-a) is above</p>
<figure>
<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g95n9q6myzj30oc0dy0tu.jpg" alt="img"><figcaption>img</figcaption>
</figure>
<p>differences: 1) e(-a) are larger 2) adaboost is approximation of max-likelihood logistic regression with larger penalty.</p>
<figure>
<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g95nakqg24j316i09cdi1.jpg" alt="img"><figcaption>img</figcaption>
</figure></li>
</ul>
<h2 id="regression-tree">2.10. regression tree</h2>
<ul>
<li><p><strong>Def</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200222122909121.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200222122917107.png"></p></li>
</ul>
<h3 id="loss-function-7">2.10.1. loss function</h3>
<ul>
<li><p><strong>Def</strong>: loss = RSS</p>
<p><img src="/2019/11/19/stats_theory/image-20200222123146373.png"></p></li>
</ul>
<h3 id="optimization-3">2.10.2. optimization</h3>
<ul>
<li><p>choose Xj &amp; separate point to make RSS smallest</p>
<p><img src="/2019/11/19/stats_theory/image-20200222123310677.png"></p></li>
<li><p>repeat</p>
<p><img src="/2019/11/19/stats_theory/image-20200222125318556.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200222130719665.png"></p></li>
<li><p>overall</p>
<p><img src="/2019/11/19/stats_theory/image-20200223164351810.png"></p></li>
<li><p>regularization</p>
<p><img src="/2019/11/19/stats_theory/image-20200222130730162.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200222130739720.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200222130754156.png"></p></li>
</ul>
<h2 id="decision-tree">2.11. decision tree</h2>
<ul>
<li><p><strong>Usage</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200222133419622.png"></p></li>
<li><p><strong>Def</strong>: same, only node value = 1/0</p></li>
</ul>
<h3 id="loss-function-8">2.11.1. loss function</h3>
<h4 id="error-rate">2.11.1.1. error rate</h4>
<ul>
<li><p><strong>Def</strong>: split into 2 to make loss1+ loss2 small. remember loss1 &amp; loss2 are all in forms below. error ate</p>
<p><img src="/2019/11/19/stats_theory/image-20200222130914042.png"></p>
<ul>
<li><p>note: how to get f: preset structure: tree model</p></li>
<li><p>note: how to get res: intuition</p></li>
<li><p>note: how to get f ha: minimize EmL = loss + weight, by spliting tree.</p></li>
</ul></li>
</ul>
<h4 id="information-gain">2.11.1.2. information gain</h4>
<ul>
<li><p><strong>Def</strong>: entrophy</p>
<p><img src="/2019/11/19/stats_theory/image-20200223152634740.png"></p>
<ul>
<li><p><strong>Qua</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200223152655567.png"></p></li>
<li><p><strong>Note</strong>: example</p>
<p><img src="/2019/11/19/stats_theory/image-20200223152805731.png"></p></li>
</ul></li>
<li><p><strong>Def</strong>: conditional entrophy</p>
<p><img src="/2019/11/19/stats_theory/image-20200223152923846.png"></p></li>
<li><p><strong>Def</strong>: information gain</p>
<p><img src="/2019/11/19/stats_theory/image-20200223153049995.png"></p>
<ul>
<li><p><strong>Note</strong>: choose node with largest information gain (when constrained in A, entrophy drops)</p>
<p><img src="/2019/11/19/stats_theory/image-20200223153218136.png"></p></li>
</ul></li>
</ul>
<h4 id="information-gain-ratio">2.11.1.3. information gain ratio</h4>
<ul>
<li><p><strong>Def</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200223155718361.png"></p></li>
</ul>
<h4 id="gini">2.11.1.4. gini</h4>
<ul>
<li><p><strong>Usage</strong>: gini is different from IG, but similar</p></li>
<li><p><strong>Def</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200223164440671.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200223155841019.png"></p>
<ul>
<li><strong>Note</strong>: the smaller the better</li>
</ul></li>
</ul>
<h4 id="cross-entrophy-no">2.11.1.5. cross entrophy (no)</h4>
<ul>
<li><p><strong>Def</strong>: cross entrophy</p>
<p><img src="/2019/11/19/stats_theory/image-20200223160203805.png"></p>
<ul>
<li><strong>Usage</strong>: since entropy is a constant, we only need to minimize cross entrophy</li>
</ul></li>
<li><p><strong>Def</strong>: KL = cross - entropy</p>
<p><img src="/2019/11/19/stats_theory/image-20200223160309988.png"></p>
<ul>
<li><strong>Usage</strong>: to determine differences between distributions</li>
</ul></li>
</ul>
<h3 id="implementation-1">2.11.2. implementation</h3>
<h4 id="id3">2.11.2.1. ID3</h4>
<ul>
<li><p><strong>Usage</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200223160953025.png"></p></li>
<li><p><strong>Def</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200223161022010.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200223161028965.png"></p>
<ul>
<li><strong>Note</strong>: that each path contains no duplicate features.</li>
</ul></li>
</ul>
<h4 id="c4.5">2.11.2.2. C4.5</h4>
<ul>
<li><p><strong>Usage</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200223161239385.png"></p></li>
<li><p><strong>Def</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200223161228973.png"></p></li>
</ul>
<h4 id="cart">2.11.2.3. CART</h4>
<ul>
<li><p><strong>Usage</strong>: binary tree!!</p>
<p><img src="/2019/11/19/stats_theory/image-20200223174438226.png"></p></li>
<li><p><strong>Def</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200223174429314.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200223174450572.png"></p></li>
</ul>
<h3 id="alternatives-improvement-1">2.11.3. alternatives &amp; improvement</h3>
<h4 id="trimming">2.11.3.1. trimming</h4>
<ul>
<li><p><strong>Usage</strong>: overfitting</p>
<p><img src="/2019/11/19/stats_theory/image-20200223161312117.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200223162719177.png"></p></li>
<li><p><strong>Def</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200223162812376.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200223162823508.png"></p>
<ul>
<li><strong>Note</strong>: start from all leaves, and trim children if tree+chidren cost more than tree, this is likely since |T| is in loss function.</li>
</ul></li>
</ul>
<h4 id="bagging-tree">2.11.3.2. bagging tree</h4>
<ul>
<li><p><strong>Def</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200222141214352.png"></p></li>
</ul>
<h4 id="random-forest">2.11.3.3. random forest</h4>
<ul>
<li><p><strong>Def</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200222141647500.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200222141702131.png"></p>
<ul>
<li><p><strong>Qua</strong>: difference with baggin</p>
<p><img src="/2019/11/19/stats_theory/image-20200222141724070.png"></p></li>
</ul></li>
</ul>
<h4 id="boosting">2.11.3.4. boosting</h4>
<h2 id="dnn-algorithm">2.12. dnn algorithm</h2>
<h3 id="regularization">2.12.1. regularization</h3>
<h1 id="unsupervised-algorithms">3. unsupervised algorithms</h1>
<h2 id="clustering-analysis">3.1. clustering analysis</h2>
<ul>
<li><strong>Usage</strong>: given =&gt; data(X1,X2,...XN), output=&gt;(X1,cluster)...(XN,cluster)</li>
</ul>
<h3 id="transformation">3.1.1. transformation</h3>
<ul>
<li><p><strong>Def</strong>:</p>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201122002694.png" alt="img"><figcaption>img</figcaption>
</figure>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201122031211.png" alt="img"><figcaption>img</figcaption>
</figure></li>
</ul>
<h3 id="distances-1">3.1.2. distances</h3>
<h4 id="between-samples">3.1.2.1. between samples</h4>
<ul>
<li><p><strong>Def</strong>:</p>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201122107491.png" alt="img"><figcaption>img</figcaption>
</figure>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201122121943.png" alt="img"><figcaption>img</figcaption>
</figure>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201122137270.png" alt="img"><figcaption>img</figcaption>
</figure></li>
</ul>
<h4 id="between-features">3.1.2.2. between features</h4>
<ul>
<li><p><strong>Def</strong>:</p>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201122348043.png" alt="img"><figcaption>img</figcaption>
</figure>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201122412699.png" alt="img"><figcaption>img</figcaption>
</figure></li>
</ul>
<h4 id="between-clusters">3.1.2.3. between clusters</h4>
<ul>
<li><p><strong>Def</strong>:</p>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201122650989.png" alt="img"><figcaption>img</figcaption>
</figure>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201122704886.png" alt="img"><figcaption>img</figcaption>
</figure>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201122713787.png" alt="img"><figcaption>img</figcaption>
</figure>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201122721151.png" alt="img"><figcaption>img</figcaption>
</figure>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201122735232.png" alt="img"><figcaption>img</figcaption>
</figure></li>
<li><p><strong>Theorem</strong>: overall:</p>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201122757600.png" alt="img"><figcaption>img</figcaption>
</figure>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201122802902.png" alt="img"><figcaption>img</figcaption>
</figure></li>
</ul>
<h3 id="hierarchical-clustering">3.1.3. hierarchical clustering</h3>
<ul>
<li><p><strong>Algorithm</strong>:</p>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201122545882.png" alt="img"><figcaption>img</figcaption>
</figure>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201122551154.png" alt="img"><figcaption>img</figcaption>
</figure>
<ul>
<li><p><strong>Qua</strong>: =&gt; distance are mono</p>
<figure>
<img src="https://i.imgur.com/cFQb1Iu.png" alt="img"><figcaption>img</figcaption>
</figure></li>
<li><p><strong>Qua</strong>: =&gt; s</p>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201123134289.png" alt="img"><figcaption>img</figcaption>
</figure></li>
</ul></li>
<li><p><strong>Qua</strong>: how to set the cluster number</p>
<p>P245</p></li>
</ul>
<h3 id="dynamic-clustering">3.1.4. dynamic clustering</h3>
<h4 id="batch-wise">3.1.4.1. batch wise</h4>
<ul>
<li><p><strong>Algorithm</strong>:</p>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201123429030.png" alt="img"><figcaption>img</figcaption>
</figure>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201123437316.png" alt="img"><figcaption>img</figcaption>
</figure></li>
</ul>
<h4 id="sample-wisekmeans">3.1.4.2. sample wise(kmeans)</h4>
<ul>
<li><p><strong>Algorithm</strong>:</p>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201123540209.png" alt="img"><figcaption>img</figcaption>
</figure>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201123550908.png" alt="img"><figcaption>img</figcaption>
</figure></li>
</ul>
<h2 id="principal-component-analysis">3.2. principal component analysis</h2>
<h3 id="pca">3.2.1. PCA</h3>
<ul>
<li><p><strong>Usage</strong>: given data(X1...XN), output datalessdimension(X'1...X'N)</p></li>
<li><p><strong>Def</strong>: principle vector</p>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201123710276.png" alt="img"><figcaption>img</figcaption>
</figure>
<p><strong>Note</strong>:</p>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201123719008.png" alt="img"><figcaption>img</figcaption>
</figure>
<ul>
<li><p><strong>Qua</strong>: ways to compute</p>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201123912143.png" alt="img"><figcaption>img</figcaption>
</figure>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201123922167.png" alt="img"><figcaption>img</figcaption>
</figure>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201123926995.png" alt="img"><figcaption>img</figcaption>
</figure></li>
<li><p><strong>Qua</strong>:</p>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201124039466.png" alt="img"><figcaption>img</figcaption>
</figure></li>
</ul></li>
<li><p><strong>Def</strong>: 因子负荷</p>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201124057423.png" alt="img"><figcaption>img</figcaption>
</figure>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201124129466.png" alt="img"><figcaption>img</figcaption>
</figure>
<ul>
<li><p><strong>Qua</strong>:</p>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201124244543.png" alt="img"><figcaption>img</figcaption>
</figure></li>
</ul></li>
<li><p><strong>Qua</strong>：</p>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201124220732.png" alt="img"><figcaption>img</figcaption>
</figure></li>
<li><p><strong>Def</strong>: total variance</p>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201124322392.png" alt="img"><figcaption>img</figcaption>
</figure></li>
<li><p><strong>Def</strong>: contribution</p>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201124336415.png" alt="img"><figcaption>img</figcaption>
</figure></li>
<li><p><strong>Def</strong>: contribution2</p>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201124425578.png" alt="img"><figcaption>img</figcaption>
</figure></li>
</ul>
<h3 id="standardized-pca">3.2.2. standardized PCA</h3>
<ul>
<li><p><strong>Def</strong>:</p>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201124518711.png" alt="img"><figcaption>img</figcaption>
</figure>
<ul>
<li><p><strong>Qua</strong>:</p>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201124541487.png" alt="img"><figcaption>img</figcaption>
</figure>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201124550283.png" alt="img"><figcaption>img</figcaption>
</figure></li>
</ul></li>
</ul>
<h3 id="sample-pca">3.2.3. sample PCA</h3>
<ul>
<li><p><strong>Def</strong>:</p>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201124651096.png" alt="img"><figcaption>img</figcaption>
</figure>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201124658605.png" alt="img"><figcaption>img</figcaption>
</figure>
<ul>
<li><p><strong>Qua</strong>:</p>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201124739337.png" alt="img"><figcaption>img</figcaption>
</figure>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201124754049.png" alt="img"><figcaption>img</figcaption>
</figure>
<figure>
<img src="/2019/11/19/stats_theory/image-20191201124759841.png" alt="img"><figcaption>img</figcaption>
</figure></li>
</ul></li>
</ul>
<h2 id="factor-analysis">3.3. factor analysis</h2>
<h2 id="canonical-correlation-analysis">3.4. canonical correlation analysis</h2>
<h2 id="em">3.5. EM</h2>
<ul>
<li><p><strong>Usage</strong>: input =&gt; data(X1,X2,...Xn), output =&gt; parameters concerning X's distribution, a step-wise &amp; approximate algorithm of max likelihood.</p>
<figure>
<img src="file:///Users/huangbenson/SteinsGate9.github.io/source/_posts/stats_theory/2020-02-25-15-11-35-image.png" alt="img"><figcaption>img</figcaption>
</figure>
<p>why can't use max likelihood? because it contains log of sum =&gt; hard to get derivative. Note that z does not appear on the left, it means that we can not observe z or other wise we can put <span class="math inline">\(p(y,z |\theta )\)</span> together and get <span class="math inline">\(p(y,z|\theta)=p(z|\theta)p(y|z,\theta)=\pi ^{z}(1-\pi) ^{1-z}p^{yz} (1-p) ^{(1-y)z}q ^{y(1-z)}(1-q) ^{(1-y)(1-z)}=(\pi p ^{y}(1-p) ^{1-y}) ^{z}...\)</span>, this is very easy to estimate via max likelihood. The general idea is that with more data, the problem will become more easy to solve.</p>
<p><img src="/2019/11/19/stats_theory/image-20200225153540498.png"></p>
<p>example of using EM algorithm</p>
<p><img src="/2019/11/19/stats_theory/image-20200225154411788.png"></p></li>
<li><p><strong>Def</strong>: Q function</p>
<p><img src="/2019/11/19/stats_theory/image-20200225153134179.png"></p></li>
<li><p><strong>Def</strong>: EM, hardest part is to get Q.</p>
<p><img src="file:///Users/huangbenson/SteinsGate9.github.io/source/_posts/stats_theory/2020-02-25-15-29-41-image.png" alt="img"><img src="file:///Users/huangbenson/SteinsGate9.github.io/source/_posts/stats_theory/2020-02-25-15-30-09-image.png" alt="img"></p>
<ul>
<li><p><strong>Note</strong>: some notices,</p>
<p><img src="/2019/11/19/stats_theory/image-20200225153219463.png"></p></li>
</ul></li>
</ul>
<h3 id="loss-function-9">3.5.1. loss function</h3>
<ul>
<li><p><strong>Def</strong>: max likelihood . This is the function we wish to maximize</p>
<p><img src="/2019/11/19/stats_theory/image-20200225154003978.png"></p>
<p>this is the function we actually maximize</p>
<p><img src="/2019/11/19/stats_theory/image-20200225162329933.png"></p>
<ul>
<li><strong>Note</strong>: f comes from preset model with parameters (e.g. normal distribution)</li>
<li><strong>Note</strong>: <span class="math inline">\(\hat{f}\)</span> comes from step-wise optimization with data.</li>
</ul></li>
</ul>
<h3 id="optimization-4">3.5.2. optimization</h3>
<ul>
<li><p><strong>Def</strong>: we want to use step-wise optimization</p>
<p><img src="/2019/11/19/stats_theory/image-20200225154028331.png"></p>
<p>getting lower bound for the differentiate by Jensen</p>
<p><img src="/2019/11/19/stats_theory/image-20200225154108953.png"></p>
<p>using lower bound to do step-wise optimization</p>
<p><img src="/2019/11/19/stats_theory/image-20200225154300172.png"></p>
<ul>
<li><p><strong>Note</strong>: illustration very good =&gt; in each round, we maximize <span class="math inline">\(B(\theta, \theta_i)\)</span> to get <span class="math inline">\(\theta_{i+1}\)</span>, then we get <span class="math inline">\(B(\theta, \theta_{i+1} )\)</span> which is another lower bound curve and we seek for peak for that. this is a non-decreasing algorithm since B is lower bound and we always seek peaks on B.</p>
<p><img src="/2019/11/19/stats_theory/image-20200225154310804.png"></p></li>
</ul></li>
</ul>
<h3 id="convergence-3">3.5.3. convergence</h3>
<ul>
<li><p><strong>Theorem</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200225154437415.png"></p></li>
<li><p><strong>Theorem</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200225154449645.png"></p>
<ul>
<li><p><strong>Note</strong>:</p>
<p><img src="/2019/11/19/stats_theory/image-20200225154515227.png"></p></li>
</ul></li>
</ul>
<h3 id="alternatives-improvements-2">3.5.4. alternatives &amp; improvements</h3>
<h4 id="gmm">3.5.4.1. GMM</h4>
<ul>
<li><p><strong>Def</strong>: GMM</p>
<p><img src="/2019/11/19/stats_theory/image-20200225154544196.png"></p>
<p>first we need to know what parameters to estimate, what hidden variables we don't know. Note that we can make up a hidden variable so that the post probability is the same =&gt; we can image we first choose randomly from 1-&gt;k and then generate result. The max likelihood function can be computed by full probability expansion/</p>
<p><img src="/2019/11/19/stats_theory/image-20200225181941793.png"></p>
<p>define Q</p>
<p><img src="/2019/11/19/stats_theory/image-20200225183525606.png"></p>
<p>expand Q</p>
<p><img src="/2019/11/19/stats_theory/image-20200225192715651.png"></p>
<p>maximize Q</p>
<p><img src="/2019/11/19/stats_theory/image-20200225192738095.png"></p>
<p><img src="/2019/11/19/stats_theory/image-20200225192745259.png"></p>
<p>overall</p>
<p><img src="/2019/11/19/stats_theory/image-20200225192730646.png"></p></li>
</ul>
<h2 id="markov-algorithm">3.6. markov algorithm</h2>
<h1 id="reinforced-algorithms">4. reinforced algorithms</h1>
<p>lecture 2基本介绍了基本前置定理比如vc之类的；</p>
<p>lecture 1先是介绍了基本算法等，然后用average和hoeffding bound介绍了EPM思想。</p>
<p>lecture 3先是介绍了基本框架，然后用EPT证明各种bound，然用用这些bound介绍EPM思想。</p>

      
    </div>
    
    
    

    <div>
      
        
<div class="my_post_copyright">
  <script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>
  
  <!-- JS库 sweetalert 可修改路径 -->
  <script src="https://cdn.bootcss.com/jquery/2.0.0/jquery.min.js"></script>
  <script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script>
  <p><span>Title:</span><a href="/2019/11/19/stats_theory/">Statistical Learning Theory</a></p>
  <p><span>Author:</span><a href="/" title=" Benson 的personal blog">Benson</a></p>
  <p><span>PTime:</span>2019/11/19 - 12:11</p>
  <p><span>LUpdate:</span>2020/02/28 - 23:02</p>
  <p><span>Link:</span><a href="/2019/11/19/stats_theory/" title="Statistical Learning Theory">https://steinsgate9.github.io/2019/11/19/stats_theory/</a>
    <span class="copy-path"  title="click to copy link"><i class="fa fa-clipboard" data-clipboard-text="https://steinsgate9.github.io/2019/11/19/stats_theory/"  aria-label="copy done！"></i></span>
  </p>
  <p><span>Protocal:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)</a> Please keep the original link and author.</p>  
</div>
<script> 
    var clipboard = new Clipboard('.fa-clipboard');
    $(".fa-clipboard").click(function(){
      clipboard.on('success', function(){
        swal({   
          title: "",   
          text: 'copy done',
          icon: "success", 
          showConfirmButton: true
          });
    });
    });  
</script>

      
    </div>
    
    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div></div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>Donate</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.png" alt="Benson WeChat Pay"/>
        <p>WeChat Pay</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.png" alt="Benson Alipay"/>
        <p>Alipay</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/numerical-stats/" rel="tag"># numerical stats</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/11/19/datastructure-algorithm/" rel="next" title="Data structure & Algorithm & Leetcode">
                <i class="fa fa-chevron-left"></i> Data structure & Algorithm & Leetcode
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/11/19/tensorflow/" rel="prev" title="Tensorflow">
                Tensorflow <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
        <!-- Go to www.addthis.com/dashboard to customize your tools -->
<div class="addthis_inline_share_toolbox">
  <script type = "text/javascript" src = "//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5dd409321289b452" async = "async" ></script>
</div>

      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="lv-container" data-id="city" data-uid="MTAyMC80NzYxNi8yNDExNA"></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="https://tva1.sinaimg.cn/large/006y8mN6gy1g96p07mbexj30uf0u0npd.jpg"
                alt="Benson" />
            
              <p class="site-author-name" itemprop="name">Benson</p>
              <p class="site-description motion-element" itemprop="description">Benson's personal blog, including stats, cs, math, among others.</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">26</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">14</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="bensonuouououow@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-globe"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://twitter.com/bensonuouououo" target="_blank" title="Twitter">
                      
                        <i class="fa fa-fw fa-globe"></i>Twitter</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.instagram.com/bensonuouououo/" target="_blank" title="Instagram">
                      
                        <i class="fa fa-fw fa-globe"></i>Instagram</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.alloyteam.com/nav/" title="Web前端导航" target="_blank">Web前端导航</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.chuangzaoshi.com/code" title="创造狮导航" target="_blank">创造狮导航</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.36zhen.com/t?id=3448" title="前端书籍资料" target="_blank">前端书籍资料</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://e.xitu.io/" title="掘金酱" target="_blank">掘金酱</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.v2ex.com/" title="V2EX" target="_blank">V2EX</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.v2ex.com/" title="印记中文" target="_blank">印记中文</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#risk-theory"><span class="nav-text">1. risk theory</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#learning-algorithm-loss-functions-1-1-3-1"><span class="nav-text">1.1. learning algorithm &amp; loss functions (1-1, 3-1)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#loss-functions"><span class="nav-text">1.2. loss functions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#e-f-loss-1-1-3-1"><span class="nav-text">1.2.1. E-f-loss (1-1, 3-1)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#e-f_--loss-plug-in-classifiers-1-1-3-2"><span class="nav-text">1.2.2. E-$f_ $-loss (plug-in classifiers) (1-1, 3-2)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#e-f-newloss-1-6-1-7"><span class="nav-text">1.2.3. E-f-newloss (1-6, 1-7)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#em-f-loss-1-213-3-345"><span class="nav-text">1.2.4. Em-f-loss (1-2,13, 3-3,4,5)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#empirical-bounds-3-345"><span class="nav-text">1.2.4.1. empirical bounds (3-3,4,5)</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#uniform-deviation-bound"><span class="nav-text">1.2.4.1.1. uniform deviation bound</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#vc-bound"><span class="nav-text">1.2.4.1.2. vc bound</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#bound-improvement-by-approximation-error"><span class="nav-text">1.2.4.1.3. bound improvement by approximation error</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#uap-approximate-error"><span class="nav-text">1.2.4.1.3.1. UAP &amp; approximate error</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#sieve-estimators"><span class="nav-text">1.2.4.1.3.2. sieve estimators</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#rate-of-convergence"><span class="nav-text">1.2.4.1.3.3. rate of convergence</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#rademacher-averages-bound-1-18"><span class="nav-text">1.2.4.1.4. Rademacher averages bound (1-18)</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#empirical-processing-theory"><span class="nav-text">1.2.4.2. empirical processing theory</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#glivenko-cantelli"><span class="nav-text">1.2.4.2.1. Glivenko-Cantelli</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#empirical-distribution-function-book-2-2"><span class="nav-text">1.2.4.2.1.1. empirical distribution function (book, 2-2)</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#gc-class-theory-1-16-2-2"><span class="nav-text">1.2.4.2.1.2. GC class &amp; theory (1-16, 2-2)</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#generalized-glivenko-cantelli-1-1314-2-6"><span class="nav-text">1.2.4.2.2. generalized Glivenko-Cantelli (1-13,14, 2-6)</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#symmetrization-lemma-2-5"><span class="nav-text">1.2.4.2.2.1. symmetrization lemma (2-5)</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#generalized-gc"><span class="nav-text">1.2.4.2.2.2. generalized GC</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#vc-class-and-stuff"><span class="nav-text">1.2.4.2.3. vc class and stuff</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#covering-number-2-7-2-8"><span class="nav-text">1.2.4.2.3.1. covering number (2-7, 2-8)</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#entropy"><span class="nav-text">1.2.4.2.3.2. entropy</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#vc-theory-for-sets3-5"><span class="nav-text">1.2.4.2.3.3. vc theory for sets(3-5)</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#mono-layer-a-special-classifier3-5"><span class="nav-text">1.2.4.2.3.4. mono layer (a special classifier)(3-5)</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#p-glivenko-cantelli-for-class-f"><span class="nav-text">1.2.4.2.4. P-Glivenko-Cantelli (for class F)</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#alternative-loss-functions"><span class="nav-text">1.3. alternative loss functions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#maximum-likelihood"><span class="nav-text">1.3.1. maximum likelihood</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#probably-approximately-correct-1-13-3-4"><span class="nav-text">1.3.2. probably approximately correct (1-13, 3-4)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#supervised-algorithms"><span class="nav-text">2. supervised algorithms</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#decision-theory"><span class="nav-text">2.1. decision theory</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#model"><span class="nav-text">2.1.1. model</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#generative-model"><span class="nav-text">2.1.1.1. generative model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#discriminative-model"><span class="nav-text">2.1.1.2. discriminative model</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#loss-function"><span class="nav-text">2.1.2. loss function</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#e-f-loss"><span class="nav-text">2.1.2.1. E f loss</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#em-f-loss"><span class="nav-text">2.1.2.2. Em f loss</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#max-likelihood"><span class="nav-text">2.1.2.3. max likelihood</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#optimization-algorithm"><span class="nav-text">2.1.3. optimization algorithm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#validation-criteria"><span class="nav-text">2.1.4. validation criteria</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#accuracy"><span class="nav-text">2.1.4.1. accuracy</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#validation-improvement"><span class="nav-text">2.1.5. validation improvement</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#validation-set"><span class="nav-text">2.1.5.1. validation set</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#leave-one-out-validation"><span class="nav-text">2.1.5.2. leave one out validation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#leave-k-out-validation"><span class="nav-text">2.1.5.3. leave k out validation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#bootstrap"><span class="nav-text">2.1.5.4. bootstrap</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#high-dimension-problem"><span class="nav-text">2.1.6. high dimension problem</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#knn-algorithm"><span class="nav-text">2.2. knn algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#loss-function-1"><span class="nav-text">2.2.1. loss function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#alternatives-improvements"><span class="nav-text">2.2.2. alternatives &amp; improvements</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#distances"><span class="nav-text">2.2.2.1. distances</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#choose-of-k"><span class="nav-text">2.2.2.2. choose of k</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#implementation"><span class="nav-text">2.2.3. implementation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#perceptron-algorithm"><span class="nav-text">2.3. perceptron algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#loss-function-2"><span class="nav-text">2.3.1. loss function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#optimization-algorithm-1"><span class="nav-text">2.3.2. optimization algorithm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#convergence"><span class="nav-text">2.3.3. convergence</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#evaluation"><span class="nav-text">2.3.4. evaluation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#support-vector-machine"><span class="nav-text">2.4. support vector machine</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#loss-fucntion"><span class="nav-text">2.4.1. loss fucntion</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#optimization"><span class="nav-text">2.4.2. optimization</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#basic"><span class="nav-text">2.4.2.1. basic</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#dual"><span class="nav-text">2.4.2.2. dual</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#alternatives-optimization"><span class="nav-text">2.4.3. alternatives &amp; optimization</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#kernel-svm"><span class="nav-text">2.4.3.1. kernel SVM</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#primitive-svm"><span class="nav-text">2.4.3.2. primitive SVM</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#soft-c-svm"><span class="nav-text">2.4.3.3. soft C-SVM</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#soft--svm"><span class="nav-text">2.4.3.4. soft $ $-SVM</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#sensitive-svm-1-10"><span class="nav-text">2.4.3.5. $ $-sensitive SVM (1-10)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#convergence-1"><span class="nav-text">2.4.4. convergence</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#relationship-with-logistic-regression"><span class="nav-text">2.4.5. relationship with logistic regression</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#discriminant-algorithm"><span class="nav-text">2.5. discriminant algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#lda-by-distance"><span class="nav-text">2.5.1. LDA: by distance</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#if-sum1-sum-2"><span class="nav-text">2.5.1.1. if \(\sum1 = \sum 2\)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#if-sum1-neq-sum2"><span class="nav-text">2.5.1.2. if \(\sum1 \neq \sum2\)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#if-more-than-1"><span class="nav-text">2.5.1.3. if more than 1</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#qda-by-general-distance"><span class="nav-text">2.5.2. QDA: by general distance</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#by-max-posterior-estimate"><span class="nav-text">2.5.3. by max posterior (estimate)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#by-bayes"><span class="nav-text">2.5.4. by bayes</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#if-sum-all-the-same-gaussian"><span class="nav-text">2.5.4.1. if \(\sum\) all the same (gaussian)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#if-not-all-the-same-gaussian"><span class="nav-text">2.5.4.2. if not all the same (gaussian)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#by-fisher"><span class="nav-text">2.5.5. by Fisher</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#r1"><span class="nav-text">2.5.5.1. r=1</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#k2"><span class="nav-text">2.5.5.1.1. k=2</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#r"><span class="nav-text">2.5.5.2. r=?</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bayes-algorithm"><span class="nav-text">2.6. bayes algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#loss-function-3"><span class="nav-text">2.6.1. loss function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#optimization-algorithm-2"><span class="nav-text">2.6.2. optimization algorithm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#alternatives-improvements-1"><span class="nav-text">2.6.3. alternatives &amp; improvements</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#laplace-smooth"><span class="nav-text">2.6.3.1. laplace smooth</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#logistic-regression-1-13"><span class="nav-text">2.7. logistic regression (1-13)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#loss-function-4"><span class="nav-text">2.7.1. loss function</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#max-entropy-algorithm"><span class="nav-text">2.8. max entropy algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#loss-function-5"><span class="nav-text">2.8.1. loss function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#optimization-1"><span class="nav-text">2.8.2. optimization</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#dual-problem"><span class="nav-text">2.8.2.1. dual problem</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#max-likelihood-1"><span class="nav-text">2.8.2.2. max likelihood</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#iis"><span class="nav-text">2.8.3. IIS</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ensemble-algorithm"><span class="nav-text">2.9. ensemble algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#boost"><span class="nav-text">2.9.1. boost</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#adaboost-algorithm"><span class="nav-text">2.9.2. adaboost algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#loss-function-6"><span class="nav-text">2.9.2.1. loss function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#optimization-2"><span class="nav-text">2.9.2.2. optimization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#convergence-2"><span class="nav-text">2.9.2.3. convergence</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#alternatives-improvement"><span class="nav-text">2.9.2.4. alternatives &amp; improvement</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#generalized"><span class="nav-text">2.9.2.4.1. generalized</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#boosting-tree"><span class="nav-text">2.9.2.4.2. boosting tree</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#gradient-boosting"><span class="nav-text">2.9.2.4.3. gradient boosting</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#relationship-with-logistic-regression-1"><span class="nav-text">2.9.2.5. relationship with logistic regression</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#regression-tree"><span class="nav-text">2.10. regression tree</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#loss-function-7"><span class="nav-text">2.10.1. loss function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#optimization-3"><span class="nav-text">2.10.2. optimization</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#decision-tree"><span class="nav-text">2.11. decision tree</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#loss-function-8"><span class="nav-text">2.11.1. loss function</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#error-rate"><span class="nav-text">2.11.1.1. error rate</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#information-gain"><span class="nav-text">2.11.1.2. information gain</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#information-gain-ratio"><span class="nav-text">2.11.1.3. information gain ratio</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#gini"><span class="nav-text">2.11.1.4. gini</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#cross-entrophy-no"><span class="nav-text">2.11.1.5. cross entrophy (no)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#implementation-1"><span class="nav-text">2.11.2. implementation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#id3"><span class="nav-text">2.11.2.1. ID3</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#c4.5"><span class="nav-text">2.11.2.2. C4.5</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#cart"><span class="nav-text">2.11.2.3. CART</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#alternatives-improvement-1"><span class="nav-text">2.11.3. alternatives &amp; improvement</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#trimming"><span class="nav-text">2.11.3.1. trimming</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#bagging-tree"><span class="nav-text">2.11.3.2. bagging tree</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#random-forest"><span class="nav-text">2.11.3.3. random forest</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#boosting"><span class="nav-text">2.11.3.4. boosting</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dnn-algorithm"><span class="nav-text">2.12. dnn algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#regularization"><span class="nav-text">2.12.1. regularization</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#unsupervised-algorithms"><span class="nav-text">3. unsupervised algorithms</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#clustering-analysis"><span class="nav-text">3.1. clustering analysis</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#transformation"><span class="nav-text">3.1.1. transformation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#distances-1"><span class="nav-text">3.1.2. distances</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#between-samples"><span class="nav-text">3.1.2.1. between samples</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#between-features"><span class="nav-text">3.1.2.2. between features</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#between-clusters"><span class="nav-text">3.1.2.3. between clusters</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hierarchical-clustering"><span class="nav-text">3.1.3. hierarchical clustering</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dynamic-clustering"><span class="nav-text">3.1.4. dynamic clustering</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#batch-wise"><span class="nav-text">3.1.4.1. batch wise</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#sample-wisekmeans"><span class="nav-text">3.1.4.2. sample wise(kmeans)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#principal-component-analysis"><span class="nav-text">3.2. principal component analysis</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#pca"><span class="nav-text">3.2.1. PCA</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#standardized-pca"><span class="nav-text">3.2.2. standardized PCA</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sample-pca"><span class="nav-text">3.2.3. sample PCA</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#factor-analysis"><span class="nav-text">3.3. factor analysis</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#canonical-correlation-analysis"><span class="nav-text">3.4. canonical correlation analysis</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#em"><span class="nav-text">3.5. EM</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#loss-function-9"><span class="nav-text">3.5.1. loss function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#optimization-4"><span class="nav-text">3.5.2. optimization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#convergence-3"><span class="nav-text">3.5.3. convergence</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#alternatives-improvements-2"><span class="nav-text">3.5.4. alternatives &amp; improvements</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#gmm"><span class="nav-text">3.5.4.1. GMM</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#markov-algorithm"><span class="nav-text">3.6. markov algorithm</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#reinforced-algorithms"><span class="nav-text">4. reinforced algorithms</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2015 &mdash; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Benson</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      total visitors
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      people
    </span>
  

  
    <span class="site-pv">
      total read
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      times
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  












  




  
  
  
  <link rel="stylesheet" href="/lib/algolia-instant-search/instantsearch.min.css">

  
  
  <script src="/lib/algolia-instant-search/instantsearch.min.js"></script>
  

  <script src="/js/src/algolia-search.js?v=5.1.4"></script>



  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.staticfile.org/MathJax/MathJax-2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
